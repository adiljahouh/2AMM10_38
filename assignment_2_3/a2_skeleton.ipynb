{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730fd591",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_2_3/a2_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {},
   "source": [
    "# Group Number: 38\n",
    "\n",
    "# Student 1:\n",
    "\n",
    "# Student 2:\n",
    "\n",
    "# Student 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {},
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0580a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adil\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0756591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_array(zipfile, fn):\n",
    "    return np.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb77a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data:\n",
      "\n",
      "positions: (10000, 4, 2, 5)\n",
      "velocities: (10000, 1, 2, 5)\n",
      "charges: (10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads the training, validation or test data as numpy arrays,\n",
    "with the positions, initial velocities and charge data of the particles.\n",
    "\n",
    "The position arrays are shaped as\n",
    "[simulation id, time point (corresponding to t = 0, 0.5, 1 or 1.5), x/y spatial dimension, particle id].\n",
    "\n",
    "The initial velocity arrays are shaped as\n",
    "[simulation id, 1 (corresponding to t=0), x/y spatial dimension, particle id].\n",
    "\n",
    "The charge arrays are shaped as [simulation id, particle id, 1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/OIgda2ZRG8v0eqB/download')\n",
    "\n",
    "features = ['positions', 'velocities', 'charges']\n",
    "    \n",
    "positions_train, velocities_train, charges_train = (load_array(data, f'data/train/{f}.npy') for f in features)\n",
    "positions_valid, velocities_valid, charges_valid = (load_array(data, f'data/valid/{f}.npy') for f in features)\n",
    "positions_test, velocities_test, charges_test = (load_array(data, f'data/test/{f}.npy') for f in features)\n",
    "\n",
    "print('Shapes of the training data:\\n')\n",
    "print(f'positions: {positions_train.shape}')\n",
    "print(f'velocities: {velocities_train.shape}')\n",
    "print(f'charges: {charges_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f56c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.379586856938328"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(positions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3ea4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of retrieving data from the arrays:\n",
      "\n",
      "\n",
      "In simulation 42 of the training set, particle 3 with charge -1.0 had coordinates [ 2.05159559 -1.46130851].\n",
      "The initial velocity of this particle was [ 0.28402364 -0.24784824].\n"
     ]
    }
   ],
   "source": [
    "print('An example of retrieving data from the arrays:\\n\\n')\n",
    "\n",
    "sim_idx = 42\n",
    "t_idx = 2  # t_idx 0, 1, 2, 3 corresponds to t=0, 0.5, 1 and 1.5 respectively\n",
    "spatial_idx = (0,1)  # corresponds to both x and y dimension\n",
    "particle_idx = 3  # corresponds to particle with index 3\n",
    "\n",
    "p = positions_train[sim_idx, t_idx, spatial_idx, particle_idx]\n",
    "v = velocities_train[sim_idx, 0, spatial_idx, particle_idx]  # note: this array contains only the inital velocity -> hence the 0\n",
    "c = charges_train[sim_idx, particle_idx, 0] \n",
    "\n",
    "print(\n",
    "    f'In simulation {sim_idx} of the training set, particle {particle_idx} with charge {c} had coordinates {p}.\\nThe initial velocity of this particle was {v}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ec4c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 2, 5)\n",
      "(10000, 1, 2, 5)\n",
      "(10000, 5, 1)\n",
      "tensor([[[-0.7878,  1.5892, -1.2906, -1.9173,  0.6539],\n",
      "         [ 0.0267, -0.4211,  2.3834, -0.5785, -0.5676],\n",
      "         [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000],\n",
      "         [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000],\n",
      "         [-1.9361,  3.6532, -4.5496, -3.0958,  0.1073],\n",
      "         [-0.2276, -2.7631,  8.2998, -3.0921, -2.8976]]], dtype=torch.float64)\n",
      "19.441097754465503\n"
     ]
    }
   ],
   "source": [
    "## THIS IS HOW THE DATASET CLASS WORKS\n",
    "## WE RESHAPE TO 3 DIMENSIONS\n",
    "## WHEN ITS (1, 2, 5) WE HAVE 2 LISTS\n",
    "## THE FIRST LIST IS THE X COORDINATE FOR [PTXD1, PTXD2, ...]\n",
    "## ETC. SECOND FOR Y COORDINATE, WE DOUBLE THE CHARGES SO THERES A CHARGE FOR X AND Y AND IT HAS\n",
    "## THE FORM (1, 2, 5)\n",
    "\n",
    "## LAST TENSOR HAS 6, 2 , 5 DIMENSION\n",
    "## FIRST TWO FOR VELOCITIES (X, Y), 3 AND 4 FOR CHARGES AND 5 AND 6 FOR POSITIONS\n",
    "print(positions_train.shape)\n",
    "print(velocities_train.shape)\n",
    "print(charges_train.shape)\n",
    "x_1 = velocities_train[0][0] #t=0 by default\n",
    "x_1 = x_1.reshape(1, 2, 5)\n",
    "x_2 = charges_train #t=0 by default\n",
    "x_2 = x_2.reshape(-1, 1, 5) # reshape from 128, 5, 1 -> 128, 1, 5\n",
    "x_2 = torch.tensor(x_2)\n",
    "x_1 = torch.tensor(x_1)\n",
    "x_2 = x_2.repeat(1, 2, 1) # 128, 2, 5\n",
    "x_2 = x_2[0].reshape(1, 2, 5)\n",
    "x_3 = torch.tensor(positions_train)\n",
    "x_3 = x_3[0, 0].view(1,2,5) # input pos of t=0\n",
    "reshaped_array = torch.cat([x_1, x_2, x_3], dim=1)\n",
    "print(reshaped_array)\n",
    "print(np.max(positions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a3438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "10000 train, 2000 validation, 2000 test simulations\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print(f'{len(positions_train)} train, {len(positions_valid)} validation, {len(positions_test)} test simulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9106543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(pos, vel):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'brown']\n",
    "    for i in range(pos.shape[-1]):\n",
    "        plt.plot(pos[0, 0, i], pos[0, 1, i], 'd', color=colors[i])\n",
    "        plt.plot(pos[-1, 0, i], pos[-1, 1, i], 'x', color=colors[i])\n",
    "        plt.plot([pos[0, 0, i], pos[0, 0, i] + vel[0, 0, i]], [pos[0, 1, i], pos[0, 1, i] + vel[0, 1, i]], '--', color=colors[i])\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.xlim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.ylim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.plot([], [], 'd', color='black', label='initial position')\n",
    "    plt.plot([], [], 'x', color='black', label='final position')\n",
    "    plt.plot([], [], '--', color='black', label='initial velocity \\ndirection and magnitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28681a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 10000)\n",
    "plot_example(positions_train[random_idx], velocities_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd17cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\"\"\"\n",
    "Get all lists of indices to shuffle the train set\n",
    "\"\"\"\n",
    "def shuffle_indices(indices):\n",
    "\n",
    "    comb_list = []\n",
    "    for x in itertools.permutations(indices):\n",
    "        comb_list.append(list(x))\n",
    "    return comb_list\n",
    "\n",
    "list_indices = shuffle_indices(indices = [0, 1, 2, 3, 4])\n",
    "print(len(list_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "04d17181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7859, 0.9596, 0.0972, 0.8639, 0.3707],\n",
       "         [0.0672, 0.2163, 0.2450, 0.2633, 0.9810]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(1, 2, 5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c0ee28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffle the train set such that we move\n",
    "each particle to a different location\n",
    "such that the model is permutation invariant\n",
    "\"\"\"\n",
    "# too difficult to implement \n",
    "\n",
    "for sim in range(positions_train.shape[0]):\n",
    "    for i in list_indices[:10]:\n",
    "        # print(positions_train[sim])\n",
    "        # print(\"\\n\")\n",
    "        # print(np.transpose(positions_train[sim, :, :, i]))\n",
    "        break\n",
    "    break   \n",
    "        \n",
    "# normalize by just dividing\n",
    "# check labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ecb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.nn.functional import normalize\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    :Input data: velocity, charges and positions array of dimensions:\n",
    "            (simulations, 4, 2, 5), positions at 4 time points for 2 spatial dimensions and 5 particles\n",
    "            (simulations, 1, 2, 5), velocities at 1 time point for 2 spatial dimensions and 5 particles\n",
    "            (simulations, 5, 1), charges for 5 time points for 1 spatial dimension (x, and y combined)\n",
    "    :output data:\n",
    "            x: (batchsize, 6, 2, 5), positions, velocities and charges at 6 time points for 2 spatial dimensions and 5 particles\n",
    "            y: (batchsize, 1, 2, 5), position at 1 time point for 2 spatial dimensions and 5 particles \n",
    "    \"\"\"\n",
    "    def __init__(self, veloc, pos, charges,time_id, norm, transform=None):\n",
    "        self.velocity = torch.FloatTensor(veloc)\n",
    "        self.charges = torch.FloatTensor(charges)\n",
    "        self.position = torch.FloatTensor(pos)\n",
    "        self.time_id = time_id\n",
    "        self.norm = norm\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_1 = self.velocity[index] #t=0 by default\n",
    "        x_2 = self.charges[index] #t=0 by default\n",
    "        x_2 = x_2.reshape(-1, 1, 5) # reshape from 128, 5, 1 -> 128, 1, 5\n",
    "        x_2 = x_2.repeat(1, 2, 1) # 128, 2, 5 so we get charges for x and y\n",
    "        x_3 = self.position[index, 0].view(1,2,5) # because we only want position at time 0\n",
    "        reshaped_array = torch.cat([x_1, x_2, x_3], dim=1)\n",
    "        y = self.position[index, self.time_id].view(1,2,5) # output pos of t=time_id\n",
    "\n",
    "        if self.norm is not False:\n",
    "            reshaped_array = normalize(reshaped_array)\n",
    "            y = normalize(y)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            reshaped_array = self.transform(reshaped_array)\n",
    "            y = self.transform(y)\n",
    "\n",
    "        return reshaped_array, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dde5ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN network for value prediction\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.layer4 = nn.Linear(64, 10)\n",
    "        # self.layer5 = nn.Linear(32, 10)\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.norm2 = nn.BatchNorm1d(64)\n",
    "        self.norm3 = nn.BatchNorm1d(10)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(F.relu(self.layer1(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(F.relu(self.layer2(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(F.relu(self.layer3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm3(F.relu(self.layer4(x)))\n",
    "        # x = F.relu(self.layer4(x))\n",
    "        # x = F.relu(self.layer5(x))\n",
    "        # x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9e9754c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading checkpoint mechanisms \n",
    "# modules adapted from https://github.com/ttchengab/One_Shot_Pytorch/blob/master/network.ipynb\n",
    "from varname import argname\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "  \"\"\"\n",
    "  Utility function for saving the model \n",
    "\n",
    "  Input\n",
    "    --save_path: path to save the model\n",
    "    --model: model to be saved\n",
    "    --optimizer: optimizer to be saved\n",
    "    --val_loss: lowest validation loss so far\n",
    "\n",
    "  Output\n",
    "    Saved model as pt file\n",
    "  \"\"\"\n",
    "  if save_path==None:\n",
    "      return\n",
    "  save_path = save_path \n",
    "  state_dict = {'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss}\n",
    "\n",
    "  torch.save(state_dict, save_path)\n",
    "  print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, save_path):\n",
    "  \"\"\"\n",
    "  Utility function to load a saved model\n",
    "  Input\n",
    "    --model: model object to load the weights into\n",
    "    --optimizer: optimizer object\n",
    "    \n",
    "  Output:\n",
    "    Validation loss\n",
    "  \"\"\"\n",
    "  # save_path = f'SiameseNetwork.pt'\n",
    "  state_dict = torch.load(save_path)\n",
    "  model.load_state_dict(state_dict['model_state_dict'])\n",
    "  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "  val_loss = state_dict['val_loss']\n",
    "  print(f'Model loaded from <== {save_path}')\n",
    "  \n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3af520ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN MODEL\n",
    "from tqdm import tqdm\n",
    "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, save_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float(\"Inf\")\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in tqdm(train_loader):\n",
    "            y = y.view(y.shape[0], 10) # first 5 values in this tensor are x pos coordinates last 5 are y  pos coordinates         \n",
    "            x = x.view(x.shape[0], 30) # first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos         \n",
    "            batch_loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_train_loss = batch_loss / len(train_loader) # batch size\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in val_loader:\n",
    "                y = y.view(y.shape[0], 10)\n",
    "                x = x.view(x.shape[0], 30)\n",
    "                val_batch_loss = 0.0\n",
    "                outputs = model(x)\n",
    "                # output_val.append(outputs)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_batch_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_batch_loss / len(val_loader) # batch size\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4b95aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models to load, continue to train loop\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from varname import nameof\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "# intialize models with weights and optimizers\n",
    "model_10000_1 = SimpleModel(input_dim=30)\n",
    "model_10000_2 = SimpleModel(input_dim=30)\n",
    "model_10000_3 = SimpleModel(input_dim=30)\n",
    "model_1000_1 = SimpleModel(input_dim=30)\n",
    "model_1000_2 = SimpleModel(input_dim=30)\n",
    "model_1000_3 = SimpleModel(input_dim=30)\n",
    "model_100_1 = SimpleModel(input_dim=30)\n",
    "model_100_2 = SimpleModel(input_dim=30)\n",
    "model_100_3 = SimpleModel(input_dim=30)\n",
    "\n",
    "optimizer_10000_1 = optim.Adam(model_10000_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_10000_2 = optim.Adam(model_10000_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_10000_3 = optim.Adam(model_10000_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_1  = optim.Adam(model_1000_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_2  = optim.Adam(model_1000_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_3  = optim.Adam(model_1000_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_1  =  optim.Adam(model_100_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_2 =   optim.Adam(model_100_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_3 =   optim.Adam(model_100_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "models = [model_100_1, model_100_2, model_100_3, model_1000_1, model_1000_2, model_1000_3, model_10000_1, model_10000_2, model_10000_3]\n",
    "optimizers = [optimizer_100_1, optimizer_100_2, optimizer_100_3, optimizer_1000_1, optimizer_1000_2, optimizer_1000_3, optimizer_10000_1, optimizer_10000_2, optimizer_10000_3]\n",
    "\n",
    "# Checking for checkpoints to load them\n",
    "model_files = [f for f in listdir(\"models\") if isfile(join(\"models\", f))]\n",
    "if len(model_files) == len(models):\n",
    "    print(\"loading model from models folder\")\n",
    "    for index, model_file in enumerate(model_files):  \n",
    "        model_location = \"models\\\\\" + model_file\n",
    "        load_checkpoint(models[index], optimizers[index], model_location)\n",
    "    \n",
    "else:\n",
    "    print(\"No models to load, continue to train loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e95af5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 55.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 50.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 51.94it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 51.60it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 50.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 50.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 49.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 50.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 53.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 51.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 54.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 49.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 52.97it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 51.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 48.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 51.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 53.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 52.62it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 54.04it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 53.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 49.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 49.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 49.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 52.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 52.97it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 49.99it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 53.32it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 49.99it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 30.83it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 53.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 43.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 49.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 47.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 52.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 53.55it/s]\n",
      "100%|██████████| 79/79 [00:01<00:00, 51.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 49.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 49.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 50.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# SKIP THIS IF YOU'RE LOADING FROM \\MODELS.\n",
    "def train_loop(samples, timeids, models, optimizers, criterion):\n",
    "    count = 0 # could do enumerate but lazy to do it 2x and add them/multiply them\n",
    "    for sample in samples:\n",
    "        for timeid in timeids:\n",
    "            val_ratio = int(sample/5)\n",
    "            train_dataset =MyDataset(velocities_train[:sample], positions_train[:sample], charges_train[:sample], time_id = timeid, norm = True, transform=None) \n",
    "            val_dataset = MyDataset(velocities_valid[:val_ratio], positions_valid[:val_ratio], charges_valid[:val_ratio], time_id = timeid, norm = True, transform=None) \n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128) \n",
    "            val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128) \n",
    "            trainloss, valloss = train(model= models[count], train_loader= train_loader, val_loader=val_loader, n_epochs=10, optimizer= optimizers[count], criterion=criterion, save_name='models\\\\NN' + '_' + str(sample) + '_' + str(timeid) + '.pt')\n",
    "            plt.figure()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.plot(trainloss, label='Train Loss')\n",
    "            plt.plot(valloss, label=\"Validation Loss\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            plt.savefig(\"images\\\\NN\" + '_' + str(sample) + '_' + str(timeid) + \".png\")\n",
    "            plt.show\n",
    "            plt.close()\n",
    "            print(count)\n",
    "            count +=1\n",
    "            \n",
    "\n",
    "samples = [100, 1000, 10000]\n",
    "timeids = [1, 2, 3]\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\") # can do MSE/MAE depends on outliers\n",
    "train_loop(samples, timeids, models, optimizers, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7033d3e",
   "metadata": {},
   "source": [
    "## Prediction + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "12205fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUR PREDICTIONS AND LABELS\n",
    "from varname import nameof\n",
    "\n",
    "from tqdm import tqdm\n",
    "def test(models, testloaders):\n",
    "    output_dict = dict()\n",
    "    with torch.no_grad():\n",
    "        models_1 = models[::3] # models for time horizon 0.5 (timeid = 1)\n",
    "        models_2 = models[1::3] # models for time horizon 1 (timeid = 2)\n",
    "        models_3 = models[2::3] # models for time horizon 1.5 (timeid = 3)\n",
    "        outputlst = ['0', '00', '000']\n",
    "        for index, model in enumerate(models_1):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            count = 0\n",
    "            for x, y in testloaders[0]:\n",
    "                y = y.view(y.shape[0], 10)# first 5 values in this tensor are x pos coordinates last 5 are y pos coordinates        \n",
    "                x = x.view(x.shape[0], 30)# first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "                count += 1\n",
    "            output_dict['model_10' + outputlst[index] + '_1_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_1_labels'] = labels\n",
    "        for index, model in enumerate(models_2):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            for x, y in testloaders[1]:\n",
    "                y = y.view(y.shape[0], 10)# first 5 values in this tensor are x pos coordinates last 5 are y pos coordinates        \n",
    "                x = x.view(x.shape[0], 30)# first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "            output_dict['model_10' + outputlst[index] + '_2_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_2_labels'] = labels\n",
    "        for index, model in enumerate(models_3):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            for x, y in testloaders[2]:\n",
    "                y = y.view(y.shape[0], 10)# first 5 values in this tensor are x pos coordinates last 5 are y pos coordinates        \n",
    "                x = x.view(x.shape[0], 30)# first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "            output_dict['model_10' + outputlst[index] + '_3_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_3_labels'] = labels\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "61092918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_100_3', 'model_1000_3', 'model_10000_3']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTES\n",
    "\n",
    "# eval turns off dropouts and since we used so many regularization techniques to get a decent model we sacrifice\n",
    "# train accuracy, with such low data this can have an even larger effect\n",
    "\n",
    "# do i take a sample of validation as well?\n",
    "# ITS OUTPUTTING TO WRONG PARTS \n",
    "models_str = [\"model_100_1\", \"model_100_2\", \"model_100_3\", \"model_1000_1\", \"model_1000_2\", \"model_1000_3\", \"model_10000_1\", \"model_10000_2\", \"model_10000_3\"]\n",
    "models_str[2::3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2e5db1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_100_1_output', 'model_100_1_labels', 'model_1000_1_output', 'model_1000_1_labels', 'model_10000_1_output', 'model_10000_1_labels', 'model_100_2_output', 'model_100_2_labels', 'model_1000_2_output', 'model_1000_2_labels', 'model_10000_2_output', 'model_10000_2_labels', 'model_100_3_output', 'model_100_3_labels', 'model_1000_3_output', 'model_1000_3_labels', 'model_10000_3_output', 'model_10000_3_labels'])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1 = MyDataset(velocities_test, positions_test, charges_test, time_id = 1, norm = True, transform=None)\n",
    "test_dataset_2 = MyDataset(velocities_test, positions_test, charges_test, time_id = 2, norm = True, transform=None)\n",
    "test_dataset_3 = MyDataset(velocities_test, positions_test, charges_test, time_id = 3, norm = True, transform=None)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_dataset_1, batch_size=128) # len(test_dataset) = 2000\n",
    "test_loader_2 = torch.utils.data.DataLoader(test_dataset_2, batch_size=128) # len(test_dataset) = 2000\n",
    "test_loader_3 = torch.utils.data.DataLoader(test_dataset_3, batch_size=128) # len(test_dataset) = 2000\n",
    "\n",
    "testloaders = [test_loader_1, test_loader_2, test_loader_3]\n",
    "test_dict = test(models, testloaders)\n",
    "test_dict.keys()\n",
    "\n",
    "# Right now we return the prediction and the labels for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d2a03769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append predictions and labels to list\n",
    "def split_output(test_output, labels):\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    act_x = []\n",
    "    act_y = []\n",
    "    for batch in test_output: # 16 batches\n",
    "        arr = batch.numpy()\n",
    "        for sim in range(arr.shape[0]): # for each index in batch\n",
    "            y = arr[sim][5:] # last 5 predictions are y pos predictions\n",
    "            x = arr[sim][:5] # take every even index\n",
    "        \n",
    "            test_x.extend(x)\n",
    "            test_y.extend(y)\n",
    "    \n",
    "    for lab_batch in labels:\n",
    "        arr_lab = lab_batch.numpy()\n",
    "        for sim in range(arr_lab.shape[0]):\n",
    "            y_lab = arr_lab[sim][5:]  # take every even index\n",
    "            x_lab = arr_lab[sim][:5] # take every even index\n",
    "        \n",
    "            act_x.extend(x_lab)\n",
    "            act_y.extend(y_lab)\n",
    "    \n",
    "    return test_x, test_y, act_x, act_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ac634507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_100_1, y_pred_100_1, x_act_100_1, y_act_100_1 = split_output(test_dict['model_100_1_output'], test_dict['model_100_1_labels'])\n",
    "x_pred_100_2, y_pred_100_2, x_act_100_2, y_act_100_2 = split_output(test_dict['model_100_2_output'], test_dict['model_100_2_labels'])\n",
    "x_pred_100_3, y_pred_100_3, x_act_100_3, y_act_100_3 = split_output(test_dict['model_100_3_output'], test_dict['model_100_3_labels'])\n",
    "x_pred_1000_1, y_pred_1000_1, x_act_1000_1, y_act_1000_1 = split_output(test_dict['model_1000_1_output'], test_dict['model_1000_1_labels'])\n",
    "x_pred_1000_2, y_pred_1000_2, x_act_1000_2, y_act_1000_2 = split_output(test_dict['model_1000_2_output'], test_dict['model_1000_2_labels'])\n",
    "x_pred_1000_3, y_pred_1000_3, x_act_1000_3, y_act_1000_3 = split_output(test_dict['model_1000_3_output'], test_dict['model_1000_3_labels'])\n",
    "x_pred_10000_1, y_pred_10000_1, x_act_10000_1, y_act_10000_1 = split_output(test_dict['model_10000_1_output'], test_dict['model_10000_1_labels'])\n",
    "x_pred_10000_2, y_pred_10000_2, x_act_10000_2, y_act_10000_2 = split_output(test_dict['model_10000_2_output'], test_dict['model_10000_2_labels'])\n",
    "x_pred_10000_3, y_pred_10000_3, x_act_10000_3, y_act_10000_3 = split_output(test_dict['model_10000_3_output'], test_dict['model_10000_3_labels'])\n",
    "# labels are the same for each horizon so a bit redundant but whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc07c9",
   "metadata": {},
   "source": [
    "## Baseline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d1b60737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formala prediction\n",
    "def baseline_prediction(t):\n",
    "    baseline_pred_x = []\n",
    "    baseline_pred_y = []\n",
    "    for sim in range(2000): # should be 2000\n",
    "        for ptxd in range(5):\n",
    "            #x_t = x_0 + v_0 * t for (x, y)\n",
    "            x_pos, y_pos = (positions_test[sim, 0, 0, ptxd]) + (velocities_test[sim, 0, 0, ptxd]*t), positions_test[sim, 0, 1, ptxd] + (velocities_test[sim, 0, 1, ptxd]*t)\n",
    "            baseline_pred_x.append(x_pos)\n",
    "            baseline_pred_y.append(y_pos)\n",
    "    return baseline_pred_x, baseline_pred_y\n",
    "# charges_test\n",
    "baseline_pred_x_1, baseline_pred_y_1 = baseline_prediction(t=0.5)\n",
    "baseline_pred_x_2, baseline_pred_y_2 = baseline_prediction(t=1)\n",
    "baseline_pred_x_3, baseline_pred_y_3 = baseline_prediction(t=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "455d0784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NN's MSE with sample size 100 with horizon 0.5 is: x = 0.5260529 y = 0.47992992\n",
      "The NN's MSE with sample size 100 with horizon 1.0 is: x = 0.5161723 y = 0.5161848\n",
      "The NN's MSE with sample size 100 with horizon 1.5 is: x = 0.51144665 y = 0.5074039\n",
      "The NN's MSE with sample size 1000 with horizon 0.5 is: x = 0.334233 y = 0.341655\n",
      "The NN's MSE with sample size 1000 with horizon 1.0 is: x = 0.33957994 y = 0.35635704\n",
      "The NN's MSE with sample size 1000 with horizon 1.5 is: x = 0.37979242 y = 0.40259326\n",
      "The NN's MSE with sample size 10000 with horizon 0.5 is: x = 0.038786493 y = 0.0372379\n",
      "The NN's MSE with sample size 10000 with horizon 1.0 is: x = 0.053446125 y = 0.048554882\n",
      "The NN's MSE with sample size 10000 with horizon 1.5 is: x = 0.06059071 y = 0.0602058\n",
      "\n",
      "\n",
      "The formula's MSE with time horizon 0.5 is: x = 4.8731730437530265 y = 4.8836007994871276\n",
      "The formula's MSE with time horizon 1.0 is: x=  6.782889049402658 y = 6.77666245296252\n",
      "The formula's MSE with time horizon 1.5 is: x = 9.1250625715874 y = 9.091671255708002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"The NN's MSE with sample size 100 with horizon 0.5 is: x =\", mean_squared_error(x_act_100_1, x_pred_100_1),  \"y =\",mean_squared_error(y_act_100_1, y_pred_100_1))\n",
    "print(\"The NN's MSE with sample size 100 with horizon 1.0 is: x =\", mean_squared_error(x_act_100_2, x_pred_100_2),  \"y =\",mean_squared_error(y_act_100_2, y_pred_100_2))\n",
    "print(\"The NN's MSE with sample size 100 with horizon 1.5 is: x =\", mean_squared_error(x_act_100_3, x_pred_100_3),  \"y =\",mean_squared_error(y_act_100_3, y_pred_100_3))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 0.5 is: x =\", mean_squared_error(x_act_1000_1, x_pred_1000_1),  \"y =\",mean_squared_error(y_act_1000_1, y_pred_1000_1))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 1.0 is: x =\", mean_squared_error(x_act_1000_2, x_pred_1000_2),  \"y =\",mean_squared_error(y_act_1000_2, y_pred_1000_2))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 1.5 is: x =\", mean_squared_error(x_act_1000_3, x_pred_1000_3),  \"y =\",mean_squared_error(y_act_1000_3, y_pred_1000_3))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 0.5 is: x =\", mean_squared_error(x_act_10000_1, x_pred_10000_1),  \"y =\",mean_squared_error(y_act_10000_1, y_pred_10000_1))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 1.0 is: x =\", mean_squared_error(x_act_10000_2, x_pred_10000_2),  \"y =\",mean_squared_error(y_act_10000_2, y_pred_10000_2))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 1.5 is: x =\", mean_squared_error(x_act_10000_3, x_pred_10000_3),  \"y =\",mean_squared_error(y_act_10000_3, y_pred_10000_3))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The formula's MSE with time horizon 0.5 is: x =\" ,mean_squared_error(x_act_100_1, baseline_pred_x_1), \"y =\", mean_squared_error(y_act_100_1, baseline_pred_y_1))\n",
    "print(\"The formula's MSE with time horizon 1.0 is: x= \" ,mean_squared_error(x_act_100_2, baseline_pred_x_2), \"y =\", mean_squared_error(y_act_100_2, baseline_pred_y_2))\n",
    "print(\"The formula's MSE with time horizon 1.5 is: x =\" ,mean_squared_error(x_act_100_3, baseline_pred_x_3), \"y =\", mean_squared_error(y_act_100_3, baseline_pred_y_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb3b29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix normalize to be same\n",
    "# fix data augmentation to be same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068baf21",
   "metadata": {},
   "source": [
    "## RNN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c956ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTmodel RNN\n",
    "#####\n",
    "#https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n",
    "####\n",
    "class LSTMModel(nn.Module):\n",
    "    # input dim are the features which should be only 1 right t=0?\n",
    "    # or is it 5 because 5 particles?\n",
    "    def __init__(self, input_dim = 30 , hidden_dim = 256, layer_dim = 2, output_dim = 10, dropout_prob = 0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a33d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN MODEL\n",
    "from tqdm import tqdm\n",
    "def trainRNN(model, train_loader, n_epochs, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        output_val = [] #get output of last epoch in val\n",
    "        model.train()\n",
    "\n",
    "        for x, y in tqdm(train_loader):\n",
    "            y = y.view(y.shape[0], 10)\n",
    "            x = x.view(x.shape[0], 1, 30)\n",
    "            batch_loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, y)\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_train_loss = batch_loss / len(train_loader) # batch size\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in val_loader:\n",
    "                y = y.view(y.shape[0], 10)\n",
    "                x = x.view(x.shape[0], 1, 30)  \n",
    "                val_batch_loss = 0.0\n",
    "                outputs = model(x)\n",
    "                output_val.append(outputs)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_batch_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_batch_loss / len(val_loader) # batch size\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "    return train_losses, val_losses, output_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8240f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\") # can do MSE/MAE depends on outliers\n",
    "RNN_network = LSTMModel()\n",
    "optimizer = optim.Adam(RNN_network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "trainloss, valloss, output_val = trainRNN(model= RNN_network, train_loader=train_loader, n_epochs=10, optimizer= optimizer, criterion=loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(trainloss, label='Train Loss')\n",
    "plt.plot(valloss, label=\"Validation Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f11070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# plt.xlabel('simulations')\n",
    "# plt.ylabel('position at t=0.5')\n",
    "# fig.set_size_inches(10, 7)\n",
    "# plt.plot(x_pred[::50], label='NN x prediction') \n",
    "# plt.plot(y_pred[::50], label='NN y predication') \n",
    "# plt.plot(baseline_pred_x[::50], label='Formula x prediction') # not normalized\n",
    "# plt.plot(baseline_pred_y[::50], label='Formula y prediction') # not normalized\n",
    "# plt.plot(x_act[::50], label='x actual ')\n",
    "# plt.plot(y_act[::50], label='y actual')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# PROBLEM: DATA IS NORMALIZED BUT BASELINE PREDICTIONS ARENT"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4158c455f6fbd238463d225e32374cd628fb465de0e99af1b601eff60f49c402"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
