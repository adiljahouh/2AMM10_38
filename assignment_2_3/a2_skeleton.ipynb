{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730fd591",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_2_3/a2_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {},
   "source": [
    "# Group Number: 38\n",
    "\n",
    "# Student 1: Adil Jahouh\n",
    "\n",
    "# Student 2: Anvitha Thirthapura\n",
    "\n",
    "# Student 3: Srinidhi Ilango"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {},
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0756591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_array(zipfile, fn):\n",
    "    return np.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb77a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data:\n",
      "\n",
      "positions: (10000, 4, 2, 5)\n",
      "velocities: (10000, 1, 2, 5)\n",
      "charges: (10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads the training, validation or test data as numpy arrays,\n",
    "with the positions, initial velocities and charge data of the particles.\n",
    "\n",
    "The position arrays are shaped as\n",
    "[simulation id, time point (corresponding to t = 0, 0.5, 1 or 1.5), x/y spatial dimension, particle id].\n",
    "\n",
    "The initial velocity arrays are shaped as\n",
    "[simulation id, 1 (corresponding to t=0), x/y spatial dimension, particle id].\n",
    "\n",
    "The charge arrays are shaped as [simulation id, particle id, 1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/OIgda2ZRG8v0eqB/download')\n",
    "\n",
    "features = ['positions', 'velocities', 'charges']\n",
    "    \n",
    "positions_train, velocities_train, charges_train = (load_array(data, f'data/train/{f}.npy') for f in features)\n",
    "positions_valid, velocities_valid, charges_valid = (load_array(data, f'data/valid/{f}.npy') for f in features)\n",
    "positions_test, velocities_test, charges_test = (load_array(data, f'data/test/{f}.npy') for f in features)\n",
    "\n",
    "print('Shapes of the training data:\\n')\n",
    "print(f'positions: {positions_train.shape}')\n",
    "print(f'velocities: {velocities_train.shape}')\n",
    "print(f'charges: {charges_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c3ea4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of retrieving data from the arrays:\n",
      "\n",
      "\n",
      "In simulation 42 of the training set, particle 3 with charge -1.0 had coordinates [ 2.05159559 -1.46130851].\n",
      "The initial velocity of this particle was [ 0.28402364 -0.24784824].\n"
     ]
    }
   ],
   "source": [
    "print('An example of retrieving data from the arrays:\\n\\n')\n",
    "\n",
    "sim_idx = 42\n",
    "t_idx = 2  # t_idx 0, 1, 2, 3 corresponds to t=0, 0.5, 1 and 1.5 respectively\n",
    "spatial_idx = (0,1)  # corresponds to both x and y dimension\n",
    "particle_idx = 3  # corresponds to particle with index 3\n",
    "\n",
    "p = positions_train[sim_idx, t_idx, spatial_idx, particle_idx]\n",
    "v = velocities_train[sim_idx, 0, spatial_idx, particle_idx]  # note: this array contains only the inital velocity -> hence the 0\n",
    "c = charges_train[sim_idx, particle_idx, 0] \n",
    "\n",
    "print(\n",
    "    f'In simulation {sim_idx} of the training set, particle {particle_idx} with charge {c} had coordinates {p}.\\nThe initial velocity of this particle was {v}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10a3438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "10000 train, 2000 validation, 2000 test simulations\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print(f'{len(positions_train)} train, {len(positions_valid)} validation, {len(positions_test)} test simulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9106543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(pos, vel):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'brown']\n",
    "    for i in range(pos.shape[-1]):\n",
    "        plt.plot(pos[0, 0, i], pos[0, 1, i], 'd', color=colors[i])\n",
    "        plt.plot(pos[-1, 0, i], pos[-1, 1, i], 'x', color=colors[i])\n",
    "        plt.plot([pos[0, 0, i], pos[0, 0, i] + vel[0, 0, i]], [pos[0, 1, i], pos[0, 1, i] + vel[0, 1, i]], '--', color=colors[i])\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.xlim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.ylim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.plot([], [], 'd', color='black', label='initial position')\n",
    "    plt.plot([], [], 'x', color='black', label='final position')\n",
    "    plt.plot([], [], '--', color='black', label='initial velocity \\ndirection and magnitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28681a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 10000)\n",
    "plot_example(positions_train[random_idx], velocities_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ecb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.nn.functional import normalize\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "      Data set class for the eventual dataloader\n",
    "      \n",
    "      Input data\n",
    "            velocity, charges and positions array of dimensions:\n",
    "            --(simulations, 4, 2, 5), positions at 4 time points for 2 spatial dimensions and 5 particles\n",
    "            --(simulations, 1, 2, 5), velocities at 1 time point for 2 spatial dimensions and 5 particles\n",
    "            --(simulations, 5, 1), charges for 5 time points for 1 spatial dimension (x, and y combined)\n",
    "      Output \n",
    "            --x: (batchsize, 6, 2, 5), positions, velocities and charges at 6 time points for 2 spatial dimensions and 5 particles\n",
    "            --y: (batchsize, 1, 2, 5), position at 1 time point for 2 spatial dimensions and 5 particles \n",
    "    \"\"\"\n",
    "    def __init__(self, veloc, pos, charges,time_id, norm, transform=None):\n",
    "        self.velocity = torch.FloatTensor(veloc)\n",
    "        self.charges = torch.FloatTensor(charges)\n",
    "        self.position = torch.FloatTensor(pos)\n",
    "        self.time_id = time_id\n",
    "        self.norm = norm\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_1 = self.velocity[index] #t=0 by default\n",
    "        x_2 = self.charges[index] #t=0 by default\n",
    "        x_2 = x_2.reshape(-1, 1, 5) # reshape from 128, 5, 1 -> 128, 1, 5\n",
    "        x_2 = x_2.repeat(1, 2, 1) # 128, 2, 5 so we get charges for x and y\n",
    "        x_3 = self.position[index, 0].view(1,2,5) # because we only want position at time 0\n",
    "        reshaped_array = torch.cat([x_1, x_2, x_3], dim=1)\n",
    "        y = self.position[index, self.time_id].view(1,2,5) # output pos of t=time_id\n",
    "\n",
    "        if self.norm is not False:\n",
    "            reshaped_array = normalize(reshaped_array)\n",
    "            y = normalize(y)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            reshaped_array = self.transform(reshaped_array)\n",
    "            y = self.transform(y)\n",
    "\n",
    "        return reshaped_array, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde5ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN network for value prediction\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.layer4 = nn.Linear(64, 10)\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.norm2 = nn.BatchNorm1d(64)\n",
    "        self.norm3 = nn.BatchNorm1d(10)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(F.relu(self.layer1(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(F.relu(self.layer2(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(F.relu(self.layer3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.norm3(F.relu(self.layer4(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9754c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading checkpoint mechanisms \n",
    "# modules adapted from https://github.com/ttchengab/One_Shot_Pytorch/blob/master/network.ipynb\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "  \"\"\"\n",
    "  Utility function for saving the model \n",
    "\n",
    "  Input\n",
    "    --save_path: path to save the model\n",
    "    --model: model to be saved\n",
    "    --optimizer: optimizer to be saved\n",
    "    --val_loss: lowest validation loss so far\n",
    "\n",
    "  Output\n",
    "    --Saved model as pt file\n",
    "  \"\"\"\n",
    "  if save_path==None:\n",
    "      return\n",
    "  save_path = save_path \n",
    "  state_dict = {'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss}\n",
    "\n",
    "  torch.save(state_dict, save_path)\n",
    "  print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, save_path):\n",
    "  \"\"\"\n",
    "  Utility function to load a saved model\n",
    "  Input\n",
    "    --model: model object to load the weights into\n",
    "    --optimizer: optimizer object\n",
    "    \n",
    "  Output\n",
    "    --Validation loss\n",
    "  \"\"\"\n",
    "  # save_path = f'SiameseNetwork.pt'\n",
    "  state_dict = torch.load(save_path)\n",
    "  model.load_state_dict(state_dict['model_state_dict'])\n",
    "  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "  val_loss = state_dict['val_loss']\n",
    "  print(f'Model loaded from <== {save_path}')\n",
    "  \n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af520ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN MODEL\n",
    "from tqdm import tqdm\n",
    "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, save_name):\n",
    "    \"\"\"\n",
    "       Training function for the model\n",
    "         Input\n",
    "            --model: model to be trained\n",
    "            --train_loader: training data loader\n",
    "            --val_loader: validation data loader\n",
    "            --n_epochs: number of epochs to train for\n",
    "            --optimizer: optimizer to be used\n",
    "            --criterion: loss function to be used\n",
    "            --save_name: name of the model to be saved\n",
    "          Output\n",
    "            --train_losses: train loss\n",
    "            --val_losses: validation loss\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float(\"Inf\")\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        # batch_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            batch_loss = 0.0\n",
    "            y = y.view(y.shape[0], 10) # first 5 values in this tensor are x pos coordinates last 5 are y  pos coordinates         \n",
    "            x = x.view(x.shape[0], 30) # first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos         \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_train_loss = batch_loss / len(train_loader) # loss per batch\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # val_batch_loss = 0.0\n",
    "            for x, y in val_loader:\n",
    "                y = y.view(y.shape[0], 10)\n",
    "                x = x.view(x.shape[0], 30)\n",
    "                val_batch_loss = 0.0\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_batch_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_batch_loss / len(val_loader) # batch size\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4b95aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models to load, continue to train loop\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from varname import nameof\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "# intialize models with weights and optimizers\n",
    "model_10000_1 = SimpleModel(input_dim=30)\n",
    "model_10000_2 = SimpleModel(input_dim=30)\n",
    "model_10000_3 = SimpleModel(input_dim=30)\n",
    "model_1000_1 = SimpleModel(input_dim=30)\n",
    "model_1000_2 = SimpleModel(input_dim=30)\n",
    "model_1000_3 = SimpleModel(input_dim=30)\n",
    "model_100_1 = SimpleModel(input_dim=30)\n",
    "model_100_2 = SimpleModel(input_dim=30)\n",
    "model_100_3 = SimpleModel(input_dim=30)\n",
    "\n",
    "optimizer_10000_1 = optim.Adam(model_10000_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_10000_2 = optim.Adam(model_10000_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_10000_3 = optim.Adam(model_10000_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_1  = optim.Adam(model_1000_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_2  = optim.Adam(model_1000_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_1000_3  = optim.Adam(model_1000_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_1  =  optim.Adam(model_100_1.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_2 =   optim.Adam(model_100_2.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_100_3 =   optim.Adam(model_100_3.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "models = [model_100_1, model_100_2, model_100_3, model_1000_1, model_1000_2, model_1000_3, model_10000_1, model_10000_2, model_10000_3]\n",
    "optimizers = [optimizer_100_1, optimizer_100_2, optimizer_100_3, optimizer_1000_1, optimizer_1000_2, optimizer_1000_3, optimizer_10000_1, optimizer_10000_2, optimizer_10000_3]\n",
    "\n",
    "# Checking for checkpoints to load them\n",
    "model_files = [f for f in listdir(\"models\") if isfile(join(\"models\", f))]\n",
    "if len(model_files) == len(models):\n",
    "    model_files = sorted(model_files)\n",
    "    correctorder = [6, 7, 8, 3, 4, 5, 0, 1 , 2] # reordering them to load them correctly\n",
    "    model_files = [model_files[i] for i in correctorder]\n",
    "    print(\"loading model from models folder\")\n",
    "    for index, model_file in enumerate(model_files):  \n",
    "        model_location = \"models\\\\\" + model_file\n",
    "        load_checkpoint(models[index], optimizers[index], model_location)\n",
    "    \n",
    "else:\n",
    "    print(\"No models to load, continue to train loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e95af5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ==> models\\NN_100_1.pt\n",
      "train loss avg  1.26616690158844\n",
      "val loss avg  0.5344330132007599\n",
      "Model saved to ==> models\\NN_100_2.pt\n",
      "train loss avg  1.2829691886901855\n",
      "val loss avg  0.5308555245399476\n",
      "Model saved to ==> models\\NN_100_3.pt\n",
      "train loss avg  1.2404412984848023\n",
      "val loss avg  0.5042475968599319\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "Model saved to ==> models\\NN_1000_1.pt\n",
      "train loss avg  0.0944747805595398\n",
      "val loss avg  0.2283508762717247\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "Model saved to ==> models\\NN_1000_2.pt\n",
      "train loss avg  0.09488905295729637\n",
      "val loss avg  0.21371229439973832\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "Model saved to ==> models\\NN_1000_3.pt\n",
      "train loss avg  0.09035011753439903\n",
      "val loss avg  0.20178544968366624\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "Model saved to ==> models\\NN_10000_1.pt\n",
      "train loss avg  0.002810652393706237\n",
      "val loss avg  0.006058815750293434\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "Model saved to ==> models\\NN_10000_2.pt\n",
      "train loss avg  0.002849531494363954\n",
      "val loss avg  0.007002205355092883\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "Model saved to ==> models\\NN_10000_3.pt\n",
      "train loss avg  0.0032640709718571434\n",
      "val loss avg  0.007686070073395968\n"
     ]
    }
   ],
   "source": [
    "# SKIP THIS IF YOU'RE LOADING FROM \\MODELS.\n",
    "from torchvision import transforms\n",
    "def train_loop(samples, timeids, models, optimizers, criterion):\n",
    "    \"\"\"\n",
    "    Train each model on their respective sample size\n",
    "    Also retrieve the correct labels for that model by timeid\n",
    "    \"\"\"\n",
    "    count = 0 \n",
    "    for sample in samples:\n",
    "        for timeid in timeids:\n",
    "            val_ratio = int(sample/5)\n",
    "            train_dataset =MyDataset(velocities_train[:sample], positions_train[:sample], charges_train[:sample], time_id = timeid, norm = True, transform=None) \n",
    "            val_dataset = MyDataset(velocities_valid[:val_ratio], positions_valid[:val_ratio], charges_valid[:val_ratio], time_id = timeid, norm = True, transform=None) \n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128) \n",
    "            val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128) \n",
    "            trainloss, valloss = train(model= models[count], train_loader= train_loader, val_loader=val_loader, n_epochs=10, optimizer= optimizers[count], criterion=criterion, save_name='models\\\\NN' + '_' + str(sample) + '_' + str(timeid) + '.pt')\n",
    "            plt.figure()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.plot(trainloss, label='Train Loss')\n",
    "            plt.plot(valloss, label=\"Validation Loss\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            plt.savefig(\"images\\\\NN\" + '_' + str(sample) + '_' + str(timeid) + \".png\", bbox_inches='tight')\n",
    "            plt.show\n",
    "            plt.close()\n",
    "            print(\"train loss avg \", sum(trainloss)/len(trainloss))\n",
    "            print(\"val loss avg \", sum(valloss)/len(valloss))\n",
    "            count+=1\n",
    "            \n",
    "\n",
    "samples = [100, 1000, 10000]\n",
    "timeids = [1, 2, 3]\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.RandomHorizontalFlip(1)\n",
    ") # flips order of ptxid to check for permutation invariance\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\") # can do MSE/MAE depends on outliers\n",
    "train_loop(samples, timeids, models, optimizers, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7033d3e",
   "metadata": {},
   "source": [
    "## Prediction + label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb3b29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12205fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUR PREDICTIONS AND LABELS\n",
    "from varname import nameof\n",
    "\n",
    "from tqdm import tqdm\n",
    "def test(models, testloaders):\n",
    "    \"\"\"\n",
    "    Testing each of the 9 models on the respective test set\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "    with torch.no_grad():\n",
    "        models_1 = models[::3] # models for time horizon 0.5 (timeid = 1)\n",
    "        models_2 = models[1::3] # models for time horizon 1 (timeid = 2)\n",
    "        models_3 = models[2::3] # models for time horizon 1.5 (timeid = 3)\n",
    "        outputlst = ['0', '00', '000']\n",
    "        for index, model in enumerate(models_1):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            count = 0\n",
    "            for x, y in testloaders[0]:\n",
    "                y = y.view(y.shape[0], 10)# first 5 values in this tensor are x pos coordinates last 5 are y pos coordinates        \n",
    "                x = x.view(x.shape[0], 30)# first 0-5 x veloc, 5-10 y veloc, 10-15 charges, 15-20 y charges, 20-25 x pos, 25-30 y pos\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "                count += 1\n",
    "            output_dict['model_10' + outputlst[index] + '_1_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_1_labels'] = labels\n",
    "        for index, model in enumerate(models_2):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            for x, y in testloaders[1]:\n",
    "                y = y.view(y.shape[0], 10)       \n",
    "                x = x.view(x.shape[0], 30)\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "            output_dict['model_10' + outputlst[index] + '_2_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_2_labels'] = labels\n",
    "        for index, model in enumerate(models_3):\n",
    "            output_test = [] \n",
    "            labels = []\n",
    "            model.eval()\n",
    "            for x, y in testloaders[2]:\n",
    "                y = y.view(y.shape[0], 10)      \n",
    "                x = x.view(x.shape[0], 30)\n",
    "                outputs = model(x)\n",
    "                output_test.append(outputs)\n",
    "                labels.append(y)\n",
    "            output_dict['model_10' + outputlst[index] + '_3_output'] = output_test \n",
    "            output_dict['model_10' + outputlst[index] + '_3_labels'] = labels\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e5db1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_100_1_output', 'model_100_1_labels', 'model_1000_1_output', 'model_1000_1_labels', 'model_10000_1_output', 'model_10000_1_labels', 'model_100_2_output', 'model_100_2_labels', 'model_1000_2_output', 'model_1000_2_labels', 'model_10000_2_output', 'model_10000_2_labels', 'model_100_3_output', 'model_100_3_labels', 'model_1000_3_output', 'model_1000_3_labels', 'model_10000_3_output', 'model_10000_3_labels'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1 = MyDataset(velocities_test, positions_test, charges_test, time_id = 1, norm = True, transform=None)\n",
    "test_dataset_2 = MyDataset(velocities_test, positions_test, charges_test, time_id = 2, norm = True, transform=None)\n",
    "test_dataset_3 = MyDataset(velocities_test, positions_test, charges_test, time_id = 3, norm = True, transform=None)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_dataset_1, batch_size=128) # len(test_dataset) = 2000\n",
    "test_loader_2 = torch.utils.data.DataLoader(test_dataset_2, batch_size=128) # len(test_dataset) = 2000\n",
    "test_loader_3 = torch.utils.data.DataLoader(test_dataset_3, batch_size=128) # len(test_dataset) = 2000\n",
    "\n",
    "testloaders = [test_loader_1, test_loader_2, test_loader_3]\n",
    "test_dict = test(models, testloaders)\n",
    "test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2a03769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append predictions and labels to list\n",
    "def split_output(test_output, labels):\n",
    "    \"\"\"\n",
    "    Splitting the output of the model into x and y coordinates\n",
    "    and splitting the labels into x and y coordinates\n",
    "    \"\"\"\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    act_x = []\n",
    "    act_y = []\n",
    "    for batch in test_output: # 16 batches\n",
    "        arr = batch.numpy()\n",
    "        for sim in range(arr.shape[0]): # for each index in batch\n",
    "            y = arr[sim][5:] # last 5 predictions are y pos predictions\n",
    "            x = arr[sim][:5] # take every even index\n",
    "        \n",
    "            test_x.extend(x)\n",
    "            test_y.extend(y)\n",
    "    \n",
    "    for lab_batch in labels:\n",
    "        arr_lab = lab_batch.numpy()\n",
    "        for sim in range(arr_lab.shape[0]):\n",
    "            y_lab = arr_lab[sim][5:]  # take every even index\n",
    "            x_lab = arr_lab[sim][:5] # take every even index\n",
    "        \n",
    "            act_x.extend(x_lab)\n",
    "            act_y.extend(y_lab)\n",
    "    \n",
    "    return test_x, test_y, act_x, act_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac634507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_100_1, y_pred_100_1, x_act_100_1, y_act_100_1 = split_output(test_dict['model_100_1_output'], test_dict['model_100_1_labels'])\n",
    "x_pred_100_2, y_pred_100_2, x_act_100_2, y_act_100_2 = split_output(test_dict['model_100_2_output'], test_dict['model_100_2_labels'])\n",
    "x_pred_100_3, y_pred_100_3, x_act_100_3, y_act_100_3 = split_output(test_dict['model_100_3_output'], test_dict['model_100_3_labels'])\n",
    "x_pred_1000_1, y_pred_1000_1, x_act_1000_1, y_act_1000_1 = split_output(test_dict['model_1000_1_output'], test_dict['model_1000_1_labels'])\n",
    "x_pred_1000_2, y_pred_1000_2, x_act_1000_2, y_act_1000_2 = split_output(test_dict['model_1000_2_output'], test_dict['model_1000_2_labels'])\n",
    "x_pred_1000_3, y_pred_1000_3, x_act_1000_3, y_act_1000_3 = split_output(test_dict['model_1000_3_output'], test_dict['model_1000_3_labels'])\n",
    "x_pred_10000_1, y_pred_10000_1, x_act_10000_1, y_act_10000_1 = split_output(test_dict['model_10000_1_output'], test_dict['model_10000_1_labels'])\n",
    "x_pred_10000_2, y_pred_10000_2, x_act_10000_2, y_act_10000_2 = split_output(test_dict['model_10000_2_output'], test_dict['model_10000_2_labels'])\n",
    "x_pred_10000_3, y_pred_10000_3, x_act_10000_3, y_act_10000_3 = split_output(test_dict['model_10000_3_output'], test_dict['model_10000_3_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc07c9",
   "metadata": {},
   "source": [
    "## Baseline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b60737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula prediction\n",
    "def baseline_prediction(t):\n",
    "    baseline_pred_x = []\n",
    "    baseline_pred_y = []\n",
    "    for sim in range(2000): # should be 2000\n",
    "        for ptxd in range(5):\n",
    "            #x_t = x_0 + v_0 * t for (x, y)\n",
    "            x_pos, y_pos = (positions_test[sim, 0, 0, ptxd]) + (velocities_test[sim, 0, 0, ptxd]*t), positions_test[sim, 0, 1, ptxd] + (velocities_test[sim, 0, 1, ptxd]*t)\n",
    "            baseline_pred_x.append(x_pos)\n",
    "            baseline_pred_y.append(y_pos)\n",
    "    return baseline_pred_x, baseline_pred_y\n",
    "\n",
    "baseline_pred_x_1, baseline_pred_y_1 = baseline_prediction(t=0.5)\n",
    "baseline_pred_x_2, baseline_pred_y_2 = baseline_prediction(t=1)\n",
    "baseline_pred_x_3, baseline_pred_y_3 = baseline_prediction(t=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "455d0784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NN's MSE with sample size 100 with horizon 0.5 is: x = 0.5318006 y = 0.52051145\n",
      "The NN's MSE with sample size 100 with horizon 1.0 is: x = 0.5597054 y = 0.54274076\n",
      "The NN's MSE with sample size 100 with horizon 1.5 is: x = 0.52472943 y = 0.50720197\n",
      "The NN's MSE with sample size 1000 with horizon 0.5 is: x = 0.35273904 y = 0.39030015\n",
      "The NN's MSE with sample size 1000 with horizon 1.0 is: x = 0.3477827 y = 0.3292618\n",
      "The NN's MSE with sample size 1000 with horizon 1.5 is: x = 0.3445897 y = 0.32860664\n",
      "The NN's MSE with sample size 10000 with horizon 0.5 is: x = 0.039533414 y = 0.041828737\n",
      "The NN's MSE with sample size 10000 with horizon 1.0 is: x = 0.051968914 y = 0.049988832\n",
      "The NN's MSE with sample size 10000 with horizon 1.5 is: x = 0.06450208 y = 0.06284643\n",
      "\n",
      "\n",
      "The formula's MSE with time horizon 0.5 is: x = 4.8731730437530265 y = 4.8836007994871276\n",
      "The formula's MSE with time horizon 1.0 is: x=  6.782889049402658 y = 6.77666245296252\n",
      "The formula's MSE with time horizon 1.5 is: x = 9.1250625715874 y = 9.091671255708002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"The NN's MSE with sample size 100 with horizon 0.5 is: x =\", mean_squared_error(x_act_100_1, x_pred_100_1),  \"y =\",mean_squared_error(y_act_100_1, y_pred_100_1))\n",
    "print(\"The NN's MSE with sample size 100 with horizon 1.0 is: x =\", mean_squared_error(x_act_100_2, x_pred_100_2),  \"y =\",mean_squared_error(y_act_100_2, y_pred_100_2))\n",
    "print(\"The NN's MSE with sample size 100 with horizon 1.5 is: x =\", mean_squared_error(x_act_100_3, x_pred_100_3),  \"y =\",mean_squared_error(y_act_100_3, y_pred_100_3))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 0.5 is: x =\", mean_squared_error(x_act_1000_1, x_pred_1000_1),  \"y =\",mean_squared_error(y_act_1000_1, y_pred_1000_1))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 1.0 is: x =\", mean_squared_error(x_act_1000_2, x_pred_1000_2),  \"y =\",mean_squared_error(y_act_1000_2, y_pred_1000_2))\n",
    "print(\"The NN's MSE with sample size 1000 with horizon 1.5 is: x =\", mean_squared_error(x_act_1000_3, x_pred_1000_3),  \"y =\",mean_squared_error(y_act_1000_3, y_pred_1000_3))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 0.5 is: x =\", mean_squared_error(x_act_10000_1, x_pred_10000_1),  \"y =\",mean_squared_error(y_act_10000_1, y_pred_10000_1))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 1.0 is: x =\", mean_squared_error(x_act_10000_2, x_pred_10000_2),  \"y =\",mean_squared_error(y_act_10000_2, y_pred_10000_2))\n",
    "print(\"The NN's MSE with sample size 10000 with horizon 1.5 is: x =\", mean_squared_error(x_act_10000_3, x_pred_10000_3),  \"y =\",mean_squared_error(y_act_10000_3, y_pred_10000_3))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The formula's MSE with time horizon 0.5 is: x =\" ,mean_squared_error(x_act_100_1, baseline_pred_x_1), \"y =\", mean_squared_error(y_act_100_1, baseline_pred_y_1))\n",
    "print(\"The formula's MSE with time horizon 1.0 is: x= \" ,mean_squared_error(x_act_100_2, baseline_pred_x_2), \"y =\", mean_squared_error(y_act_100_2, baseline_pred_y_2))\n",
    "print(\"The formula's MSE with time horizon 1.5 is: x =\" ,mean_squared_error(x_act_100_3, baseline_pred_x_3), \"y =\", mean_squared_error(y_act_100_3, baseline_pred_y_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdfce7",
   "metadata": {},
   "source": [
    "## Extra Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b4604e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NN's MAE with sample size 100 with horizon 0.5 is: x = 0.64107263 y = 0.6341152\n",
      "The NN's MAE with sample size 100 with horizon 1.0 is: x = 0.6518703 y = 0.64693975\n",
      "The NN's MAE with sample size 100 with horizon 1.5 is: x = 0.6390358 y = 0.61871284\n",
      "The NN's MAE with sample size 1000 with horizon 0.5 is: x = 0.4546674 y = 0.47560585\n",
      "The NN's MAE with sample size 1000 with horizon 1.0 is: x = 0.4553377 y = 0.4370648\n",
      "The NN's MAE with sample size 1000 with horizon 1.5 is: x = 0.44617963 y = 0.43989503\n",
      "The NN's MAE with sample size 10000 with horizon 0.5 is: x = 0.14984146 y = 0.1565982\n",
      "The NN's MAE with sample size 10000 with horizon 1.0 is: x = 0.16927804 y = 0.16457742\n",
      "The NN's MAE with sample size 10000 with horizon 1.5 is: x = 0.18648866 y = 0.18128967\n",
      "\n",
      "\n",
      "The formula's MAE with time horizon 0.5 is: x = 1.5492140859893986 y = 1.538228046278357\n",
      "The formula's MAE with time horizon 1.0 is: x=  1.8729718215784386 y = 1.8556169888360703\n",
      "The formula's MAE with time horizon 1.5 is: x = 2.209423586351536 y = 2.1872913756639982\n"
     ]
    }
   ],
   "source": [
    "## reference: https://towardsdatascience.com/comparing-robustness-of-mae-mse-and-rmse-6d69da870828\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"The NN's MAE with sample size 100 with horizon 0.5 is: x =\", mean_absolute_error(x_act_100_1, x_pred_100_1),  \"y =\",mean_absolute_error(y_act_100_1, y_pred_100_1))\n",
    "print(\"The NN's MAE with sample size 100 with horizon 1.0 is: x =\", mean_absolute_error(x_act_100_2, x_pred_100_2),  \"y =\",mean_absolute_error(y_act_100_2, y_pred_100_2))\n",
    "print(\"The NN's MAE with sample size 100 with horizon 1.5 is: x =\", mean_absolute_error(x_act_100_3, x_pred_100_3),  \"y =\",mean_absolute_error(y_act_100_3, y_pred_100_3))\n",
    "print(\"The NN's MAE with sample size 1000 with horizon 0.5 is: x =\", mean_absolute_error(x_act_1000_1, x_pred_1000_1),  \"y =\",mean_absolute_error(y_act_1000_1, y_pred_1000_1))\n",
    "print(\"The NN's MAE with sample size 1000 with horizon 1.0 is: x =\", mean_absolute_error(x_act_1000_2, x_pred_1000_2),  \"y =\",mean_absolute_error(y_act_1000_2, y_pred_1000_2))\n",
    "print(\"The NN's MAE with sample size 1000 with horizon 1.5 is: x =\", mean_absolute_error(x_act_1000_3, x_pred_1000_3),  \"y =\",mean_absolute_error(y_act_1000_3, y_pred_1000_3))\n",
    "print(\"The NN's MAE with sample size 10000 with horizon 0.5 is: x =\", mean_absolute_error(x_act_10000_1, x_pred_10000_1),  \"y =\",mean_absolute_error(y_act_10000_1, y_pred_10000_1))\n",
    "print(\"The NN's MAE with sample size 10000 with horizon 1.0 is: x =\", mean_absolute_error(x_act_10000_2, x_pred_10000_2),  \"y =\",mean_absolute_error(y_act_10000_2, y_pred_10000_2))\n",
    "print(\"The NN's MAE with sample size 10000 with horizon 1.5 is: x =\", mean_absolute_error(x_act_10000_3, x_pred_10000_3),  \"y =\",mean_absolute_error(y_act_10000_3, y_pred_10000_3))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The formula's MAE with time horizon 0.5 is: x =\" ,mean_absolute_error(x_act_100_1, baseline_pred_x_1), \"y =\", mean_absolute_error(y_act_100_1, baseline_pred_y_1))\n",
    "print(\"The formula's MAE with time horizon 1.0 is: x= \" ,mean_absolute_error(x_act_100_2, baseline_pred_x_2), \"y =\", mean_absolute_error(y_act_100_2, baseline_pred_y_2))\n",
    "print(\"The formula's MAE with time horizon 1.5 is: x =\" ,mean_absolute_error(x_act_100_3, baseline_pred_x_3), \"y =\", mean_absolute_error(y_act_100_3, baseline_pred_y_3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4158c455f6fbd238463d225e32374cd628fb465de0e99af1b601eff60f49c402"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
