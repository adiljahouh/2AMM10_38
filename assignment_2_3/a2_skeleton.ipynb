{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730fd591",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_2_3/a2_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {},
   "source": [
    "# Group Number: 38\n",
    "\n",
    "# Student 1:\n",
    "\n",
    "# Student 2:\n",
    "\n",
    "# Student 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {},
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d0580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0756591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_array(zipfile, fn):\n",
    "    return np.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb77a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data:\n",
      "\n",
      "positions: (10000, 4, 2, 5)\n",
      "velocities: (10000, 1, 2, 5)\n",
      "charges: (10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads the training, validation or test data as numpy arrays,\n",
    "with the positions, initial velocities and charge data of the particles.\n",
    "\n",
    "The position arrays are shaped as\n",
    "[simulation id, time point (corresponding to t = 0, 0.5, 1 or 1.5), x/y spatial dimension, particle id].\n",
    "\n",
    "The initial velocity arrays are shaped as\n",
    "[simulation id, 1 (corresponding to t=0), x/y spatial dimension, particle id].\n",
    "\n",
    "The charge arrays are shaped as [simulation id, particle id, 1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/OIgda2ZRG8v0eqB/download')\n",
    "\n",
    "features = ['positions', 'velocities', 'charges']\n",
    "    \n",
    "positions_train, velocities_train, charges_train = (load_array(data, f'data/train/{f}.npy') for f in features)\n",
    "positions_valid, velocities_valid, charges_valid = (load_array(data, f'data/valid/{f}.npy') for f in features)\n",
    "positions_test, velocities_test, charges_test = (load_array(data, f'data/test/{f}.npy') for f in features)\n",
    "\n",
    "print('Shapes of the training data:\\n')\n",
    "print(f'positions: {positions_train.shape}')\n",
    "print(f'velocities: {velocities_train.shape}')\n",
    "print(f'charges: {charges_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c3ea4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of retrieving data from the arrays:\n",
      "\n",
      "\n",
      "In simulation 42 of the training set, particle 3 with charge -1.0 had coordinates [ 2.05159559 -1.46130851].\n",
      "The initial velocity of this particle was [ 0.28402364 -0.24784824].\n"
     ]
    }
   ],
   "source": [
    "print('An example of retrieving data from the arrays:\\n\\n')\n",
    "\n",
    "sim_idx = 42\n",
    "t_idx = 2  # t_idx 0, 1, 2, 3 corresponds to t=0, 0.5, 1 and 1.5 respectively\n",
    "spatial_idx = (0,1)  # corresponds to both x and y dimension\n",
    "particle_idx = 3  # corresponds to particle with index 3\n",
    "\n",
    "p = positions_train[sim_idx, t_idx, spatial_idx, particle_idx]\n",
    "v = velocities_train[sim_idx, 0, spatial_idx, particle_idx]  # note: this array contains only the inital velocity -> hence the 0\n",
    "c = charges_train[sim_idx, particle_idx, 0] \n",
    "\n",
    "print(\n",
    "    f'In simulation {sim_idx} of the training set, particle {particle_idx} with charge {c} had coordinates {p}.\\nThe initial velocity of this particle was {v}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ec4c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10a3438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "10000 train, 2000 validation, 2000 test simulations\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print(f'{len(positions_train)} train, {len(positions_valid)} validation, {len(positions_test)} test simulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9106543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(pos, vel):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'brown']\n",
    "    for i in range(pos.shape[-1]):\n",
    "        plt.plot(pos[0, 0, i], pos[0, 1, i], 'd', color=colors[i])\n",
    "        plt.plot(pos[-1, 0, i], pos[-1, 1, i], 'x', color=colors[i])\n",
    "        plt.plot([pos[0, 0, i], pos[0, 0, i] + vel[0, 0, i]], [pos[0, 1, i], pos[0, 1, i] + vel[0, 1, i]], '--', color=colors[i])\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.xlim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.ylim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.plot([], [], 'd', color='black', label='initial position')\n",
    "    plt.plot([], [], 'x', color='black', label='final position')\n",
    "    plt.plot([], [], '--', color='black', label='initial velocity \\ndirection and magnitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d28681a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 10000)\n",
    "plot_example(positions_train[random_idx], velocities_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ecb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Dataloader for velocity, charges, position in 1 dataset class\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, veloc, pos, charges):\n",
    "        self.velocity = torch.FloatTensor(veloc)\n",
    "        self.charges = torch.FloatTensor(charges)\n",
    "        self.position = torch.FloatTensor(pos)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        x = self.velocity[index]\n",
    "        y = self.charges[index]\n",
    "        z = self.position[index]\n",
    "        return x, y, z\n",
    "        # outputs list of 3 128 batches\n",
    "        # so [tensor(128, 1 , 4, 5), tensor(128, 1, 5)....]\n",
    "    def __len__(self):\n",
    "        return len(self.charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8633eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyDataset(velocities_train, positions_train, charges_train)\n",
    "val_dataset = MyDataset(velocities_valid, positions_valid, charges_valid)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a99a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 2, 5]) torch.Size([128, 4, 2, 5]) torch.Size([128, 5, 1])\n",
      "veloc {sim, init_vel, x or y, ptxid} -  charge {sim, ptxid, charge} - pos {sim, time, x or y, ptxd}\n",
      "initial velocity tensor([-0.7878,  0.0267])\n",
      "initial position tensor([-1.9361, -0.2276])\n",
      "[128, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "for veloc, charges, position in train_loader:\n",
    "    print(veloc.shape, position.shape, charges.shape)\n",
    "    print(\"veloc {sim, init_vel, x or y, ptxid} -  charge {sim, ptxid, charge} - pos {sim, time, x or y, ptxd}\")\n",
    "    print(\"initial velocity\",  veloc[0, 0, :, 0])\n",
    "    print(\"initial position\", position[0, 0, :, 0])\n",
    "    print(list(charges.shape))\n",
    "    # we need to predict from init velocity\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66774050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0267)\n",
      "tensor(1.3331)\n",
      "tensor(-0.6657)\n",
      "tensor(1.3261)\n",
      "tensor(-0.0553)\n",
      "tensor(0.7356)\n",
      "tensor(0.3854)\n",
      "tensor(0.6183)\n",
      "tensor(-0.6418)\n",
      "tensor(-1.5861)\n",
      "tensor(1.3055)\n",
      "tensor(0.2554)\n",
      "tensor(0.3869)\n",
      "tensor(-0.1922)\n",
      "tensor(-0.7341)\n",
      "tensor(1.9666)\n",
      "tensor(0.1123)\n",
      "tensor(-0.0763)\n",
      "tensor(-0.0293)\n",
      "tensor(-0.6898)\n",
      "tensor(0.1163)\n",
      "tensor(0.5168)\n",
      "tensor(1.0266)\n",
      "tensor(-0.2235)\n",
      "tensor(1.1158)\n",
      "tensor(1.1962)\n",
      "tensor(-1.3441)\n",
      "tensor(-0.6416)\n",
      "tensor(-2.3449)\n",
      "tensor(0.6827)\n",
      "tensor(-1.3712)\n",
      "tensor(1.0296)\n",
      "tensor(-0.5284)\n",
      "tensor(0.9241)\n",
      "tensor(0.1104)\n",
      "tensor(-0.8981)\n",
      "tensor(-0.8375)\n",
      "tensor(0.5335)\n",
      "tensor(-0.1515)\n",
      "tensor(0.8329)\n",
      "tensor(-0.0212)\n",
      "tensor(-0.7082)\n",
      "tensor(-0.6007)\n",
      "tensor(-0.9645)\n",
      "tensor(-1.0657)\n",
      "tensor(0.1578)\n",
      "tensor(0.4936)\n",
      "tensor(-0.4201)\n",
      "tensor(0.1239)\n",
      "tensor(0.0419)\n",
      "tensor(-0.3220)\n",
      "tensor(0.5188)\n",
      "tensor(1.6574)\n",
      "tensor(-0.2569)\n",
      "tensor(-0.0693)\n",
      "tensor(-0.1995)\n",
      "tensor(-0.7778)\n",
      "tensor(0.4720)\n",
      "tensor(0.0492)\n",
      "tensor(-0.5965)\n",
      "tensor(-1.1331)\n",
      "tensor(0.4480)\n",
      "tensor(-0.9137)\n",
      "tensor(0.7330)\n",
      "tensor(-0.6635)\n",
      "tensor(-0.5232)\n",
      "tensor(-0.3465)\n",
      "tensor(-1.1411)\n",
      "tensor(-0.6512)\n",
      "tensor(-1.9982)\n",
      "tensor(-0.1168)\n",
      "tensor(0.9989)\n",
      "tensor(0.3657)\n",
      "tensor(0.1711)\n",
      "tensor(0.2230)\n",
      "tensor(0.6245)\n",
      "tensor(-0.5506)\n",
      "tensor(1.5346)\n",
      "tensor(0.1357)\n"
     ]
    }
   ],
   "source": [
    "#todo\n",
    "# pass veloc, pos from t =0  to predict\n",
    "count = 0 \n",
    "for veloc, charges, position in train_loader:\n",
    "    v = veloc[0, 0, 1, 0]\n",
    "    # p = position[0, 0, 1, 0]\n",
    "    # c = charges[0, 0, 0]\n",
    "    # print(torch.cat([v.view(1), p.view(1), c.view(1)], dim=0).shape)\n",
    "    # print(torch.tensor([p]))\n",
    "\n",
    "    size = list(veloc.shape)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dde5ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN network for value prediction\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,3)\n",
    "        self.layer2 = nn.Linear(3, 3)\n",
    "        self.layer3 = nn.Linear(3, 1)\n",
    "        # self.drop = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        # x = self.drop(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # x = self.drop(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {},
   "source": [
    "# Model Training REGULAR NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3af520ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN MODEL\n",
    "def train(model, train_loader, n_epochs, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for veloc, charges, position in train_loader:\n",
    "            size = list(veloc.shape)\n",
    "            for sim in range(size[0]): # batch size per batch\n",
    "                pt_loss = 0.0\n",
    "                for ptxid in range(5):\n",
    "                    for coordinate_type in range(2):\n",
    "                        init_veloc_xy = veloc[sim, 0, coordinate_type, ptxid]\n",
    "                        init_pos_xy = position[sim, 0, coordinate_type, ptxid] \n",
    "                        charge = charges[sim, ptxid, 0]\n",
    "                        x = torch.cat([init_pos_xy.view(1), init_veloc_xy.view(1), charge.view(1)], dim=0) # append pos, veloc, charge to 1 tensor\n",
    "                        y = torch.tensor([position[sim, 1, coordinate_type, ptxid]])\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(x)\n",
    "                        loss = criterion(outputs, y)\n",
    "                        pt_loss += loss.item()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                pt_loss = pt_loss/10\n",
    "                running_loss += pt_loss\n",
    "                    \n",
    "        avg_train_loss = running_loss / (len(train_loader) * 128) # batch size\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for veloc, charges, position in val_loader:\n",
    "                size = list(veloc.shape)\n",
    "                for sim in range(size[0]):\n",
    "                    val_pt_loss = 0.0\n",
    "                    for ptxid in range(5):\n",
    "                        for coordinate_type in range(2):\n",
    "                            init_veloc_xy = veloc[sim, 0, coordinate_type, ptxid] # list\n",
    "                            init_pos_xy = position[sim, 0, coordinate_type, ptxid] #list\n",
    "                            charge = charges[sim, ptxid, 0]\n",
    "                            x = torch.cat([init_pos_xy.view(1), init_veloc_xy.view(1), charge.view(1)], dim=0)\n",
    "                            y = torch.tensor([position[sim, 1, coordinate_type, ptxid]])\n",
    "                            outputs = model(x)\n",
    "                            loss = criterion(outputs, y)\n",
    "                            val_pt_loss += loss.item()\n",
    "                    avg_pt_loss = val_pt_loss/10\n",
    "                    val_running_loss += avg_pt_loss\n",
    "        avg_val_loss = val_running_loss / (len(val_loader) * 128) # batch size\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e95af5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\") # can do MSE/MAE depends on outliers\n",
    "model = SimpleModel(input_dim=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainloss, valloss = train(model= model, train_loader= train_loader, n_epochs=1, optimizer= optimizer, criterion=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07e03ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.593978354389448]\n",
      "[7.25305592070695]\n"
     ]
    }
   ],
   "source": [
    "print(trainloss)\n",
    "print(valloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068baf21",
   "metadata": {},
   "source": [
    "## RNN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c956ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTmodel RNN\n",
    "# NOT FINISHED\n",
    "#####\n",
    "#https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n",
    "####\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_dim = 3 #len(X_train.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\") # can do MSE/MAE depends on outliers\n",
    "RNN_network = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim, dropout)\n",
    "optimizer = optim.Adam(RNN_network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb3b29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280031f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8240f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4158c455f6fbd238463d225e32374cd628fb465de0e99af1b601eff60f49c402"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
