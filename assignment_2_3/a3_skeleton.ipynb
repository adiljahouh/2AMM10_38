{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963690b2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_2_3/a3_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8459f1",
   "metadata": {},
   "source": [
    "# Group Number:\n",
    "\n",
    "# Student 1: Adil \n",
    "\n",
    "# Student 2: Anvitha\n",
    "\n",
    "# Student 3: Srinidhi Ilango"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde28458",
   "metadata": {},
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce00edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_pickle(zipfile, fn):\n",
    "    return pickle.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/cwqGaS22KXgnXtg/download')\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "simulation_{train, valid, test} is stored as a list of simulations. \n",
    "Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.\n",
    "\"\"\"\n",
    "simulation_train = load_pickle(data, 'data/train/simulation.pickle')  # 3.1 + 3.2\n",
    "simulation_valid = load_pickle(data, 'data/valid/simulation.pickle')  # 3.1 + 3.2\n",
    "simulation_test = load_pickle(data, 'data/test/simulation.pickle')  # 3.1 + 3.2\n",
    "\n",
    "\"\"\"\n",
    "charges_{train, valid, test} is stored as a list of simulation-charges. \n",
    "These charges are stored as numpy arrays of size (3,): One value for each charge.\n",
    "\"\"\"\n",
    "charges_train = load_pickle(data, 'data/train/charges.pickle')  # 3.1\n",
    "charges_valid = load_pickle(data, 'data/valid/charges.pickle')  # 3.1\n",
    "charges_test = load_pickle(data, 'data/test/charges.pickle')  # 3.1\n",
    "\n",
    "\"\"\"\n",
    "simulation_continued_{train, valid, test} is stored as a list of simulations. \n",
    "Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.\n",
    "\"\"\"\n",
    "simulation_continued_train = load_pickle(data, 'data/train/simulation_continued.pickle')  # 3.2\n",
    "simulation_continued_valid = load_pickle(data, 'data/valid/simulation_continued.pickle')  # 3.2\n",
    "simulation_continued_test = load_pickle(data, 'data/test/simulation_continued.pickle')  # 3.2\n",
    "\n",
    "\"\"\"\n",
    "Note that the indices are shared throughout the different lists, e.g., for the 4th training simulation:\n",
    "simulation_train[3] contains its initial simulation\n",
    "charges_train[3] contains the charges associated with the simulation\n",
    "simulation_continued_train[3] contains the continuation of the simulation \n",
    "                --> simulation_continued_train[3][0] is the state after simulation_train[3][-1]\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a3438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "Task 3.1:\n",
      "800 train, 100 validation, 100 test simulations\n",
      "800 train, 100 validation, 100 test charge pairs\n",
      "\n",
      "Task 3.2:\n",
      "Since len(simulation_continued_train) < len(simulation_train), we can only use a subset of initial simulations\n",
      "We cut simulation_train down to the first 150 samples in simulation_train_task32\n",
      "150 train, 100 validation, 100 test simulations\n",
      "150 train, 100 validation, 100 test continuations\n",
      "\n",
      "For task 3.1, use:\n",
      "simulation_train + charges_train\n",
      "simulation_valid + charges_valid\n",
      "simulation_test + charges_test\n",
      "\n",
      "For task 3.2, use:\n",
      "simulation_train_task32 + simulation_continued_train\n",
      "simulation_valid + simulation_continued_valid\n",
      "simulation_test + simulation_continued_test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print('Task 3.1:')\n",
    "print(f'{len(simulation_train)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')\n",
    "print(f'{len(charges_train)} train, {len(charges_valid)} validation, {len(charges_test)} test charge pairs')\n",
    "print()\n",
    "\n",
    "print('Task 3.2:')\n",
    "print('Since len(simulation_continued_train) < len(simulation_train), we can only use a subset of initial simulations')\n",
    "print('We cut simulation_train down to the first 150 samples in simulation_train_task32')\n",
    "simulation_train_task32 = simulation_train[:150]\n",
    "print(f'{len(simulation_train_task32)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')\n",
    "print(f'{len(simulation_continued_train)} train, {len(simulation_continued_valid)} validation, {len(simulation_continued_test)} test continuations')\n",
    "\n",
    "print(f\"\"\"\n",
    "For task 3.1, use:\n",
    "{chr(10).join([\"simulation_{} + charges_{}\".format(t, t) for t in [\"train\", \"valid\", \"test\"]])}\n",
    "\n",
    "For task 3.2, use:\n",
    "{chr(10).join([\"simulation_{} + simulation_continued_{}\".format(*((t[0], t[1]) if isinstance(t, tuple) else (t, t))) for t in [(\"train_task32\", \"train\"), \"valid\", \"test\"]])}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfafdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print some shapes:\n",
      "\n",
      "simulation_train[0].shape: (103, 2) -> (t, 2), (x, y) at every t)\n",
      "charges_train[0].shape: (3,) -> charges for the simulation\n",
      "simulation_continued_train[0].shape: (54, 2) -> (t, 2), (x, y) at every t)\n",
      "----\n",
      "\n",
      "simulation_train[1].shape: (97, 2) -> (t, 2), (x, y) at every t)\n",
      "charges_train[1].shape: (3,) -> charges for the simulation\n",
      "simulation_continued_train[1].shape: (45, 2) -> (t, 2), (x, y) at every t)\n",
      "----\n",
      "\n",
      "simulation_train[2].shape: (99, 2) -> (t, 2), (x, y) at every t)\n",
      "charges_train[2].shape: (3,) -> charges for the simulation\n",
      "simulation_continued_train[2].shape: (47, 2) -> (t, 2), (x, y) at every t)\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Print some shapes:\\n')\n",
    "for i in range(3):\n",
    "    print('simulation_train[{}].shape:'.format(i), simulation_train[i].shape, '-> (t, 2), (x, y) at every t)')\n",
    "    print('charges_train[{}].shape:'.format(i), charges_train[i].shape, '-> charges for the simulation')\n",
    "    print('simulation_continued_train[{}].shape:'.format(i), simulation_continued_train[i].shape, '-> (t, 2), (x, y) at every t)')\n",
    "    print('----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9106543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(x, x_gt=None, x_pred=None, fn=None):\n",
    "    charge_locations = np.array([[-1.53846154, -1.53846154],\n",
    "                                 [ 1.53846154, -1.53846154],\n",
    "                                 [ 0.        ,  1.53846154]])  # charge locations are fixed\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    cmap = matplotlib.cm.get_cmap('tab20')\n",
    "    plt.plot(x[:, 0], x[:, 1], color=cmap(0))\n",
    "    plt.plot(x[0, 0], x[0, 1], 'd', color=cmap(1))\n",
    "    fig.set_size_inches(5, 5)\n",
    "    for charge in charge_locations:\n",
    "        plt.plot(charge[0], charge[1], 'd', color='black')\n",
    "    if x_gt is not None:\n",
    "        plt.plot(x_gt[:, 0], x_gt[:, 1], color='red', linewidth=.5)\n",
    "    if x_pred is not None:\n",
    "        plt.plot(x_pred[:, 0], x_pred[:, 1], color='green', linestyle='--')\n",
    "    if fn is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28681a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEvCAYAAAA6m2ZKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAX60lEQVR4nO3de3Cc9X3v8fdXq8tqV/c7ti6+crGNE0AYEmhpgZOhNpRp55xJT5qeU0yHdKaZEw5pmSY0vaQzhaZDk7TktMMkbjsTZmjSNkmJybRw2k5DCBcZMCBz8Q1fhC3J1n11Xe23f6xQcDBgrEd6pP19XjMaW7vL83x3QW9+z+6zK3N3REQKXVHcA4iILAXFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCMVx7LShocHXrFkTx65FpIDt2bPnlLs3nu26WGK3Zs0aurq64ti1iBQwMzvybtfpMFZEgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCJHFzswSZva8mX0/qm2KiEQlypXdZ4BXItyeiEhkIomdmbUCO4CvR7E9EZGoRbWy+wpwN5CLaHsiIpFacOzM7Gagz933vM/t7jCzLjPr6u/vX+huRUQ+kChWdtcAv2hmbwAPA9eb2Td/+kbu/qC7d7p7Z2NjYwS7FRE5dwuOnbt/zt1b3X0N8CvAv7n7Jxc8mYhIhHSenYgEoTjKjbn7fwD/EeU2RUSioJWdiARBsRORICh2IhIExU5EgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiARBsRORICh2IhIExU5EgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJwoJjZ2ZtZvbvZrbPzLrN7DNRDCYiEqXiCLaRBT7r7s+ZWSWwx8wec/d9EWxbRCQSC17ZufsJd39u7u+jwCvA6oVuV0QkSpE+Z2dma4DLgKej3K6IyEJFFjszqwD+EbjT3UfOcv0dZtZlZl39/f1R7VZE5JxEEjszKyEfuofc/Z/Odht3f9DdO929s7GxMYrdioicsyhejTXgG8Ar7v7nCx9JRCR6UazsrgF+DbjezF6Y+9oewXZFRCKz4FNP3P0JwCKYRURk0egdFCISBMVORIKg2IlIEBQ7Wba6u7vZsmUL3d3dcY8iBUCxk2Upk8mwfft29u3bx44dO8hkMnGPJCucYifL0s6dO+nr68Pd6e3t5fbbb497JFnhFDtZdnbt2sXu3buZnJwEYHJykkceeYRdu3bFPJmsZObuS77Tzs5O7+rqWvL9ysrQ3NxMX1/fOy5vamqit7c3holkpTCzPe7eebbrtLKTZefee+8lnU6fcVkqleK+++6LaSIpBIqdLDs7d+5kx44dJJNJAJLJJLfccgu33XZbzJPJSqbYybK0a9cumpqaMDOam5v5xje+EfdIssIpdrIspdNpHn30UTZt2sTu3bvfcVgr8kFF8TsoRBbF5s2befnll+MeQwqEVnYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiARBsRORIOikYpEVYGwqS0nCKCtOgDv090NPD5w8mf/74CBkMjA9nb++rAxqa2H1arjkEli/HorCXtsodiLLSHY2x8H+DPtODPPKiVFePTnKgd5R3hye5O92buO6kSPwyCPQ2AitrXDBBXDhhfmwVVRASUl+Q9PT+QD29MBTT8Hf/i0UF8Nv/Aa0tcV6H+Oi2InExN05cnqcF44N8cKxIV48PsS+EyNMzuQAKE0UsaGpgm1r69jYXElHXQou7ITOs35c25nKyqClJf91xRX5yzIZ+NM/hRtugOuuW8R7tjwpdiJLZCo7y4vHh+l6Y5A9RwZ57uggA5lpAFKlCbasquYT2zq4tLWKTRdUs64xTUkiwkPPdBq++EW46y649lpIJKLb9gqg2IkskonpWfYcGeTpw6d5+vAALxwbYjqbX7Wta0hz/cVNXN5ey2XtNWxsqqA4yrC9l61b4cgRWLduafa3TCh2IhGZnJnl+aND/PjgKZ48eJq9x4eYmXUSRcaWVVX8r6s72La2jis6aqmvKItv0MFBqK6Ob/8xUexEztNsznmpZ5gfHTjFkwdP0fXGIFPZHEUGl7bWcPu167h6XR2da+qoKFtGP2onTkB9fdxTLLll9G9AZHl76wWFHx44xRP7+/nxwdOMTGYBuLilkl+9qoNrNtRz5do6qpIlMU/7Lv7zP+HKK+OeIhaKnch7GBqf5kcHTvPEgX5+uP8UxwcnAFhVneSmLS1cs6GBj65voLEyxsPSczUxAf/wD/DVr8Y9SSwUO5G3mc7meO7oID/c388T+0/xYs8w7lBZVsxH1tdzx8+u49oNDaxtSGNmcY977tzhj/4I7r4bVtLcEVLsJGjuzuu9Yzwxd2j69OEBxqdnSRQZH26r4f9cv5GfvbCBD7XWLN2rpYvhL/4Cbr45fyJyoBQ7Cc6J4Ql+dOA0PzpwiicOnKJ/dArInw7y369o5doNDVy9vn75Pu/2Qf3N30B7e/7cuoApdlLwBjLTPHXoNE/OnRJyqD8DQH26lI9uaOBnNjRwzcYGVteUxzzpIti1K38y8S/9UtyTxE6xk4IzND7N04cHeOrQaZ46NMArJ0YASJcm2La2jk9sa+eaDQ1c1FxJUVGBPn/lDg88kD9sVegAxU4KQN/IJM+8McCzhwd4+vAAr/WO4g7JkiIub6/ltz92IR9Z38DW1upo3361XM3Owh//Mfz8zwf5Hth3o9jJipLLOfv7xthzJP/+0q4jAxw5PQ5AeUmCyztquOvSC7l6fT1bW6vzH4kUkpER+P3fh9/8Tbj44rinWVYUO1nWTo9Nsff4EC8cHeL5uU8HGZ07kbc+XcrlHbV88qoOrlxbx+ZVVWGs3N7N66/DX/4l/MEfQEND3NMsO4qdLBvDEzN09wzzYs8wLx0f5sWeIY4N5E/iLTK4sLmSm7euorOjlis6aumoT62sc90W03e/Cy+9BF/+cv5z6+Qd9KjIknN3To5M8uqJUbrfHKb7zRG63xzh6MD4/G3a6srZurqGX72qgw+31XDp6mrSy+n9pcvF9HT+M+o2b4YvfCHuaZY1/dcjiyozleX13lFeO5n/1N3XTo7yyskRhsZn5m/TUZ/i0tXVfPzKNjavqmJraw116dIYp15eRsazPLN/iG0ba6hKve1H9vBhuP9+uPNO2LAhtvlWCsVOIjE2leVA3xgH+sbY3zvK672j7O8bm38vKeRfQLiwpZJf2NLCJRdUcckFVVzUUlk4J+8uguys8+Srg0xM5/jxq4Pc8KEGihMGDz+cf47u/vvzn0os70uxk3Pm7vSOTHGof4yD/WMc7M9wsD8fuBPDk/O3K00Usa4xzWXttXy8s40LWyq5uKWSttpU4Z7XFiF3Z2bWmZ7NsffQTz6mfXImx97nj3LFt/8f3Hhj/lVXOWeKnbxDZirL4VMZDp3KcKh/jEP9mfz3/WNkpmfnb1dRVsz6xjRXr6tnQ1PF/FdHXWplv480ArmcMzA+zamxKU6NTnM6M8XQ+AwDmWmGxqcZnphhZDLLyMQMY1NZMtNZMlOzTEzPMpWdJeewrb2eX97aPn/6TP0zP6Tmqcc5+nufp/2S9pjv4cqj2AVqNue8OTTBwbmYHTo192d/hpMjP1mlmcGq6nLWNab5H51trG9Ms66xgnWNaVqqkkG+GjqVneXk8CRvDk1yYniCE8P5P3tHpugdmaR3ZJJTY9PM5vys/3xVspjqVAnV5SVUJUtoT6eoKCsmVZagvCRBsiRBWXERLeUVJKyIxESGjd/8GmPtG3jx039IWaYIpe6DiyR2ZnYT8FUgAXzd3e+LYruycFPZWQ71Z9g/93zawf4xDvaNcfhUhqm534cA+R/AdY0VfHR9Pevmgra2Ic3ahjTJkrBOzJ3O5ugZmuDYwDjHBsc5NjDB8cFxeoYm6BmcoH9sCv+pjtWkSmipStJUleSi5kqaqsporCijobKMhooyGipKqU2VUl1ecs6r3jf6xjn+ncdp+bfvs/+Tv8VkYwuJItjcXrEI97rwLTh2ZpYAvgb8N+A48KyZ/bO771votuXcuTvHByfYd2KEV06M8NrJUV7rHeXI6fH5FUaRQVtdivWNFfzMxgbWNVawfm6VVp8uDWaV5u4Mjs9w5HSGowPjHD09nv9zYJxjA+OcGJk8I2YlCWNVTTmtteX83EWNrK5Jsaomyeqaclqqk1xQXU55acT/QxgbY80DX2K2YhUv/98vksMoMmipKaOjKRXtvgIRxcpuG3DA3Q8BmNnDwK2AYreIhidm2HNkgOeO5N9V8FLPMMMT+dM5zKCjLsVFLZXsuPQCNjZXsrGpIqhV2sxsjjeHJjhy+icRO3J6nCNzfx+byp5x+8bKMjrqUly9rp62ulT+q7actroUzVVJEkv5wspjj8EPfgC/8zt0NLWwf+8pJqZz+ff6rg/vF+VEJYrYrQaOve3748BVEWxXfsqxgXG+90IPj7/Sx4vHh8g5JIqMi1sq2X5pC5tXVbN5Vf50jlRpYT8dO5tz+kYnOT6YP8Q8NnDmYeeJ4Qne/pRZaaKI1rpyOupSXLW2jra6FB11KdrrU7TVpqJfmZ2PgQH40pfyvwT7/vvBjGLgoxfXzp9nV5wIY/W9GJbsJ8LM7gDuAGhv19OrH8TMbI4vfPdl/r7rGO7w4bYaPn39Rq5eV8dlbbXL4wc1Qu7O8MQMJ4YnOTk8Of8CQM/QBG8OTcy/MDAze+YTZ02VZbTVpbhyTS1tdatpr0vlv+pTNFcml+9pL+7wrW/B3r35j02vqzvj6qpUMTd+SO91XagoYtcDtL3t+9a5y87g7g8CDwJ0dnae/WUqOasnD57m4WePUVlWzNf/dyfb1tatuOfX3J2xqSwDmWlOZ6YZGMufltE/OpX/c2yK3pEp+kYn6R2Zmv9l0m9JFBktVUlW1SS5rL2GHTUX0FpbTmttitbaclbXlK/MQ/Q33oCvfAVuvRX+5E/inqagRRG7Z4GNZraWfOR+BfhEBNuVOZe313DV2jqePjzAxx98iuryEtY3pmmtTdFSnaShopS6dBk15SVUJotJlxWTKk2QKi0mWVJEWXGCkoSRKLJziqS7k/P8inJ6Nsd0NsdUNsfkzOz818R0jvHpLOPTs4xNZfPnik1lGZ07d2xkcobhiRmGxmcYmphheHyG6dncWfdXlSymobKM5sokV7TX0lSVpKmyjFXzLwAkaawoK6xz97JZ+Ku/gkwG7rsPksm4Jyp4C46du2fN7NPAv5A/9WSXu3cveDKZV5ks4e8/9REO9I3y47lP3j3cn+H5Y4P0dr9zFfReiouMoqL8K3uGzf+iqdxc4HI5J/su54e9nyLLz1qZLKYqWUJVeTEbmiqoSZVQXV5KXbqEunQZ9elS6tKlc6dllIb3mXPPPAMPPQSf+hRs2hT3NMGI5Dk7d38UeDSKbcm729BUyYamyjMuc3dGp7IMZvJn5Y9O5ldXEzP5Vdd0NsfkTI6Z2RzZ2RzZ3FzU3PG58yt87oUOLB/DRFERCTNKi4soSeT/LCsumj/ZNVmSXzXmV48JKpLFVJQVU16SWHGH10tqeDj/wsPatfmPYioqoJXqClDYL9kFwMzyqyi9mX75codvfxuefx7uugsaG+OeKEiKnchim5qC5ma49964Jwma1tEiiy2Z1C++WQYUOxEJgmInIkFQ7EQkCIqdiARBsRORICh2IhIExU5EgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiARBsRORICh2IhIExS5A3d3dbNmyhe7u7rhHKRh6TJc/xS4wmUyG7du3s2/fPnbs2EEmk4l7pBVPj+nKoNgFZufOnfT19eHu9Pb2cvvtt8c90oqnx3RlUOwCsmvXLnbv3s3k5CQAk5OTPPLII+zatSvmyVYuPaYrh7n7ku+0s7PTu7q6lny/oWtubqavr+8dlzc1NdHb2xvDRCufHtPlxcz2uHvn2a7Tyi4g9957L+l0+ozLUqkU9913X0wTrXx6TFcOxS4gO3fuZMeOHSSTSQCSySS33HILt912W8yTrVx6TFcOHcYGJpPJsGnTJo4dO0Z7ezvd3d3vWJnIB6PHdPnQYazMS6fTPProo2zatIndu3frhzICekxXBq3sRKRgaGUnIsFT7EQkCAuKnZn9mZm9amYvmtl3zKwmorlERCK10JXdY8AWd98KvA58buEjiYhEb0Gxc/d/dffs3LdPAa0LH0lEJHpRPme3E/hBhNsTEYlM8fvdwMweB1rOctU97v69udvcA2SBh95jO3cAdwC0t7ef17AiIufrfWPn7je+1/Vm9uvAzcAN/h4n7bn7g8CDkD/P7oONKSKyMO8bu/diZjcBdwPXuft4NCOJiERvoc/ZPQBUAo+Z2Qtm9tcRzCQiErkFrezcfUNUg4iILCa9g0JEgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiARBsRORICh2IhIExU5EgqDYiUgQFDsRCYJiJyJBUOxEJAiKnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiAQhktiZ2WfNzM2sIYrtiYhEbcGxM7M24GPA0YWPIyKyOKJY2X0ZuBvwCLYlIrIoFhQ7M7sV6HH3vRHNIyKyKIrf7wZm9jjQcpar7gE+T/4Q9n2Z2R3AHQDt7e0fYEQRkYUz9/M7+jSzS4H/D4zPXdQKvAlsc/eT7/XPdnZ2eldX13ntV0Tk3ZjZHnfvPNt177uyezfu/hLQ9LadvAF0uvup892miMhi0Xl2IhKE817Z/TR3XxPVtkREoqaVnYgEQbETkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBEGxE5EgKHYiEgTFTkSCoNiJSBAUOxEJgmInIkFQ7EQkCIqdiARBsRORICh2IhIExU5EgqDYiUgQFDsRCYK5+9Lv1KwfOLKEu2wACvmXdxfy/Svk+wa6f1HrcPfGs10RS+yWmpl1uXtn3HMslkK+f4V830D3bynpMFZEgqDYiUgQQondg3EPsMgK+f4V8n0D3b8lE8RzdiIioazsRCRwwcXOzD5rZm5mDXHPEhUz+zMze9XMXjSz75hZTdwzRcHMbjKz18zsgJn9btzzRMnM2szs381sn5l1m9ln4p4pamaWMLPnzez7cc8CgcXOzNqAjwFH454lYo8BW9x9K/A68LmY51kwM0sAXwN+AdgE/E8z2xTvVJHKAp91903A1cBvFdj9A/gM8ErcQ7wlqNgBXwbuBgrqiUp3/1d3z859+xTQGuc8EdkGHHD3Q+4+DTwM3BrzTJFx9xPu/tzc30fJR2F1vFNFx8xagR3A1+Oe5S3BxM7MbgV63H1v3LMssp3AD+IeIgKrgWNv+/44BRSDtzOzNcBlwNMxjxKlr5BfWORinmNecdwDRMnMHgdaznLVPcDnyR/Crkjvdd/c/Xtzt7mH/OHRQ0s5m5w/M6sA/hG4091H4p4nCmZ2M9Dn7nvM7OdiHmdeQcXO3W882+VmdimwFthrZpA/zHvOzLa5+8klHPG8vdt9e4uZ/TpwM3CDF8b5RD1A29u+b527rGCYWQn50D3k7v8U9zwRugb4RTPbDiSBKjP7prt/Ms6hgjzPzszeADrdvSDegG1mNwF/Dlzn7v1xzxMFMysm/2LLDeQj9yzwCXfvjnWwiFj+/7p/Bwy4+50xj7No5lZ2v+3uN8c8SjjP2RW4B4BK4DEze8HM/jrugRZq7gWXTwP/Qv7J+28VSujmXAP8GnD93L+zF+ZWQrJIglzZiUh4tLITkSAodiISBMVORIKg2IlIEBQ7EQmCYiciQVDsRCQIip2IBOG/AJ7YeKOBv1/vAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charges are [-0.09808531 -0.5563076  -0.58221579]\n"
     ]
    }
   ],
   "source": [
    "test_idx = np.random.randint(150)\n",
    "plot_example(simulation_train[test_idx], simulation_continued_train[test_idx])\n",
    "print(f'Charges are {charges_train[test_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566bfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading checkpoint mechanisms \n",
    "# modules adapted from https://github.com/ttchengab/One_Shot_Pytorch/blob/master/network.ipynb\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "  \"\"\"\n",
    "  Utility function for saving the model \n",
    "\n",
    "  Input\n",
    "    --save_path: path to save the model\n",
    "    --model: model to be saved\n",
    "    --optimizer: optimizer to be saved\n",
    "    --val_loss: lowest validation loss so far\n",
    "\n",
    "  Output\n",
    "    Saved model as pt file\n",
    "  \"\"\"\n",
    "  if save_path==None:\n",
    "      return\n",
    "  save_path = save_path \n",
    "  state_dict = {'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss}\n",
    "\n",
    "  torch.save(state_dict, save_path)\n",
    "  print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "  \"\"\"\n",
    "  Utility function to load a saved model\n",
    "  Input\n",
    "    --model: model object to load the weights into\n",
    "    --optimizer: optimizer object\n",
    "    \n",
    "  Output:\n",
    "    Validation loss\n",
    "  \"\"\"\n",
    "  save_path = f'SiameseNetwork.pt'\n",
    "  state_dict = torch.load(save_path)\n",
    "  model.load_state_dict(state_dict['model_state_dict'])\n",
    "  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "  val_loss = state_dict['val_loss']\n",
    "  print(f'Model loaded from <== {save_path}')\n",
    "  \n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883762b1",
   "metadata": {},
   "source": [
    "# Task 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ddabe",
   "metadata": {},
   "source": [
    "## Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe48be",
   "metadata": {},
   "source": [
    "Here, we deliberated over one of the following two options as the simulations have different lengths:\n",
    "1. zero padding to match the simulation lenghts\n",
    "2. without zero padding, pass one simuation at a time into the network and aggregate the outputs for backprop [ref1](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/12#:~:text=2%20MONTHS%20LATER-,GalAvineri,-cdjhz), [ref2](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch)\n",
    "\n",
    "We now try the second option <br>\n",
    "-- the model should be invariant to rotation, not sure about the implementation\n",
    "[REF](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd9df856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo \n",
    "#1: can add data normalization later\n",
    "#2: data augmentation - slice through longer time points as the outputs must remain same\n",
    "#3: data augmentation - time reveral should not affect the result (#data-pts*2)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "def collate_batch(batch):\n",
    "    '''\n",
    "    Args:\n",
    "        batch: batch sizes list of tuples (features, target)\n",
    "    \n",
    "    Returns:\n",
    "        packed input simulation and corresponding targets\n",
    "    \n",
    "    '''\n",
    "    data = [torch.from_numpy(item[0]).float() for item in batch]\n",
    "    lengths = [d.size(0) for d in data]\n",
    "\n",
    "    padded_data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    packed_data = pack_padded_sequence(padded_data, lengths, batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "    targets = np.array([item[1] for item in batch])\n",
    "\n",
    "    return packed_data.to(device), torch.tensor(targets).float().to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec1e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a DataLoaders for training, validation and testing\n",
    " \n",
    "batch_len = 8\n",
    "\n",
    "train_loader = DataLoader(tuple(zip(simulation_train, charges_train)), batch_size = batch_len, \n",
    "                        collate_fn = collate_batch)#,pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(tuple(zip(simulation_valid, charges_valid)), batch_size = batch_len, \n",
    "                        collate_fn = collate_batch)#,pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(tuple(zip(simulation_test, charges_train)),batch_size=1,\n",
    "                        collate_fn = collate_batch)#,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af0e89",
   "metadata": {},
   "source": [
    "### workshpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_batch(batch):\n",
    "#     # batch cointains a list of tuples of structure (sequence, target)\n",
    "#     data = [torch.from_numpy(item[0]) for item in batch]\n",
    "#     # print(len(data))\n",
    "#     data = pack_sequence(data, enforce_sorted=False)\n",
    "#     # print(len(data))\n",
    "#     targets = np.array([item[1] for item in batch])\n",
    "#     return data.to(device), torch.tensor(targets).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0161c84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for features,targets in test_loader:\n",
    "    print(len(features))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8853f6",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5841",
   "metadata": {},
   "source": [
    "[ref1](https://stackoverflow.com/questions/44643137/how-do-you-use-pytorch-packedsequence-in-code)\n",
    "[ref2](https://www.crosstab.io/articles/time-series-pytorch-lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8794a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: clean up code, understand it better\n",
    "#can add normalizatoin, dropouts, regularization\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class RegressionLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model to find the charge of three particles\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size=2, hidden_size=110, n_layers=1, batch_size=8, dropout=0.5):\n",
    "        super(RegressionLSTM, self).__init__()\n",
    "        \n",
    "        self.inp_size = inp_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.drop = nn.Dropout(p= dropout)        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = self.inp_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=self.hidden_size, out_features=3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h0 = torch.zeros(self.n_layers, self.batch_size ,self.hidden_size).requires_grad_()\n",
    "        # c0 = torch.zeros(self.n_layers, self.batch_size, self.hidden_size).requires_grad_()\n",
    "        \n",
    "        _,(hn,_) = self.rnn(x)#,(h0,c0))\n",
    "        last_hidden_layer = hn[0]\n",
    "        last_hidden_layer = self.drop(last_hidden_layer)\n",
    "        h = self.fc1(last_hidden_layer)\n",
    "        h = nn.ReLU()(h)\n",
    "\n",
    "        output = self.fc2(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd2d70",
   "metadata": {},
   "source": [
    "### rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62b5aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0025,  0.0249, -0.1600],\n",
      "        [-0.0624,  0.0888, -0.0651],\n",
      "        [-0.0359,  0.0848, -0.0493],\n",
      "        [-0.0468,  0.1455,  0.0264],\n",
      "        [-0.0571,  0.0753, -0.0907],\n",
      "        [-0.0366,  0.1009, -0.0218],\n",
      "        [-0.0115,  0.1144,  0.0309],\n",
      "        [-0.0021,  0.0560, -0.0602]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9951, -0.4482, -0.2730],\n",
      "        [-0.9845, -0.1668, -0.3479],\n",
      "        [-0.0918, -0.3541, -0.6817],\n",
      "        [-0.8102, -0.6751, -0.1659],\n",
      "        [-0.7157, -0.8646, -0.9235],\n",
      "        [-0.1375, -0.0528, -0.9531],\n",
      "        [-0.4891, -0.5396, -0.5350],\n",
      "        [-0.1714, -0.3899, -0.2192]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "num_layers = 1\n",
    "hidden_size = 110\n",
    "input_size = 2\n",
    "batch_size = 3\n",
    "rnn = nn.LSTM(input_size =2, hidden_size = 110, num_layers = 1, batch_first = True)\n",
    "# lSTM output is (batch_size,seq_len, num_directions * hidden_size)\n",
    "lstm = nn.LSTM(input_size, hidden_size, batch_first=True).to(device)\n",
    "linear = nn.Linear(in_features= hidden_size, out_features=3).to(device)\n",
    "\n",
    "for batch, target in train_loader:\n",
    "    \n",
    "    # seq_len, batch_size\n",
    "    h0 = torch.zeros(num_layers, batch_size ,hidden_size).requires_grad_()\n",
    "    c0 = torch.zeros(num_layers, batch_size, hidden_size).requires_grad_()\n",
    "    # hidden = (h0,c0)\n",
    "    # out,_ = rnn(batch,h0)\n",
    "    o, (h0, c0) = lstm(batch)#,(h0,c0))\n",
    "    out = linear(h0[0])#.flatten()\n",
    "    print(out)\n",
    "    print(target)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5192ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "# https://stackoverflow.com/questions/44643137/how-do-you-use-pytorch-packedsequence-in-code\n",
    "\n",
    "\n",
    "# data = [torch.tensor([1]),\n",
    "#         torch.tensor([2, 3, 4, 5]), \n",
    "#         torch.tensor([6, 7]),\n",
    "#         torch.tensor([8, 9, 10])]\n",
    "\n",
    "data = [torch.tensor(data).float() for data in simulation_train[:2]]\n",
    "lengths = [d.size(0) for d in data]\n",
    "\n",
    "padded_data = pad_sequence(data, batch_first=True, padding_value=0) \n",
    "padded_data.required_grad = True\n",
    "# embedding = nn.Embedding(20, 4, padding_idx=0)\n",
    "# embeded_data = embedding(padded_data)\n",
    "\n",
    "packed_data = pack_padded_sequence(padded_data, lengths, batch_first=True, enforce_sorted=False)\n",
    "lstm = nn.LSTM(input_size=2, hidden_size=3, batch_first=True)\n",
    "o, (h, c) = lstm(packed_data)\n",
    "\n",
    "# (h, c) is the needed final hidden and cell state, with index already restored correctly by LSTM.\n",
    "# but o is a PackedSequence object, to restore to the original index:\n",
    "\n",
    "unpacked_o, unpacked_lengths = pad_packed_sequence(o, batch_first=True)\n",
    "# now unpacked_o, (h, c) is just like the normal output you expected from a lstm layer.\n",
    "\n",
    "print(unpacked_o, unpacked_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e443b7f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54d99d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainng and validation for every epoch\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs, loss_fn, save_name):\n",
    "\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for X, y in tqdm(train_loader):\n",
    "            output = model(X)\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss/ len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for X,y in val_loader:\n",
    "                output = model(X)\n",
    "                loss = loss_fn(output, y)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_running_loss/ len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "        print('Epoch [{}/{}], train_loss: {:.4f}, val_loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print('Finished Training!!')\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a5b0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 89.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], train_loss: 0.0031, val_loss: 0.0309\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 111.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], train_loss: 0.0029, val_loss: 0.0299\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 121.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], train_loss: 0.0028, val_loss: 0.0289\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 107.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], train_loss: 0.0027, val_loss: 0.0279\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 81.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], train_loss: 0.0026, val_loss: 0.0269\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 110.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], train_loss: 0.0025, val_loss: 0.0257\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 98.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], train_loss: 0.0023, val_loss: 0.0243\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 119.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], train_loss: 0.0022, val_loss: 0.0226\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 123.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], train_loss: 0.0020, val_loss: 0.0204\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 109.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], train_loss: 0.0018, val_loss: 0.0174\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 102.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], train_loss: 0.0015, val_loss: 0.0128\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 117.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], train_loss: 0.0010, val_loss: 0.0072\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 86.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], train_loss: 0.0008, val_loss: 0.0069\n",
      "Model saved to ==> test_run1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 98.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], train_loss: 0.0008, val_loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], train_loss: 0.0008, val_loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 84.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], train_loss: 0.0008, val_loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 103.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], train_loss: 0.0007, val_loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 84.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], train_loss: 0.0006, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 94.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], train_loss: 0.0008, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 73.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], train_loss: 0.0006, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 108.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], train_loss: 0.0007, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 96.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], train_loss: 0.0009, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 90.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], train_loss: 0.0007, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 114.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], train_loss: 0.0010, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 101.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], train_loss: 0.0009, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 115.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], train_loss: 0.0006, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 105.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], train_loss: 0.0008, val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 99.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], train_loss: 0.0006, val_loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 96.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], train_loss: 0.0008, val_loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 95.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], train_loss: 0.0008, val_loss: 0.0074\n",
      "Finished Training!!\n"
     ]
    }
   ],
   "source": [
    "#todo\n",
    "from tqdm import tqdm \n",
    "\n",
    "lr = 1e-5\n",
    "hidden_units = 16\n",
    "\n",
    "model = RegressionLSTM(batch_size=batch_len)\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "num_epochs = 30\n",
    "\n",
    "save_path = 'test_run1.pt'\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, \n",
    "                                num_epochs, loss_fn, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22c5bd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02678372997504014, 0.025884041419396035]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses \n",
    "# train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "080c7a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting training vs validation loss\n",
    "\n",
    "#maybe add more layers to the model, or decrease the learning rate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f947d",
   "metadata": {},
   "source": [
    "### workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58348edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "lr = 1e-5\n",
    "hidden_units = 16\n",
    "\n",
    "model = RegressionLSTM(batch_size=batch_len)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "total_loss = 0\n",
    "# num_batches = \n",
    "model.train()\n",
    "\n",
    "for X,y in tqdm(train_loader):\n",
    "    output = model(X)\n",
    "    loss = loss_fn(output,y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X,y in val_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "176b3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_running_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X,y in val_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "    val_running_loss += loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da139d5b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebed03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo : evaluate the model performance on the test set\n",
    "#check the training example it performs the worst on \n",
    "#try to obtain possible reasons for such performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af7ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3422e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a59808b",
   "metadata": {},
   "source": [
    "# Task 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a855d",
   "metadata": {},
   "source": [
    "## Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b29ecd",
   "metadata": {},
   "source": [
    "We create a `collate_batch` method which produces batches of source and target sentences. This method will be used in the 'DataLoader' which enables us to iterate over the dataset in batches. In each iteration, a batch of input and output simulations would be returned. We pad all the sequences to the length of the longest simulation in the batch then we pack the padded simulation for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0ec19a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_32(batch):\n",
    "    '''\n",
    "    Args:\n",
    "        batch: batch sizes list of tuples (features, target)\n",
    "    \n",
    "    Returns:\n",
    "        packed input simulation and corresponding targets\n",
    "    \n",
    "    '''\n",
    "    data = [torch.from_numpy(item[0]).float() for item in batch]\n",
    "    data_lengths = [d.size(0) for d in data]\n",
    "\n",
    "    padded_data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    # packed_data = pack_padded_sequence(padded_data, data_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # targets = np.array([item[1] for item in batch],dtype=object)\n",
    "    targets = [torch.from_numpy(item[1]).float() for item in batch]\n",
    "    target_lengths = [t.size(0) for t in targets]\n",
    "    padded_target = pad_sequence(targets, padding_value= 0)\n",
    "    # packed_target = pack_padded_sequence(padded_target, target_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return padded_data.to(device), padded_target.to(device) \n",
    "\n",
    "    # return packed_data.to(device), packed_target.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5b935865",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_len = 10\n",
    "train_loader_32 = DataLoader(tuple(zip(simulation_train_task32, simulation_continued_train[:])), batch_size = batch_len, \n",
    "                        collate_fn = collate_batch_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00b12",
   "metadata": {},
   "source": [
    "### workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "20a53997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch, target in train_loader_32:\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f189d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(len(batch[1][0]))\n",
    "    # print(batch[1][2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a5e9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = list(zip(simulation_train_task32[:3],simulation_continued_train[:3]))\n",
    "\n",
    "data = [torch.from_numpy(item[0]).float() for item in batch]\n",
    "lengths = [d.size(0) for d in data]\n",
    "\n",
    "padded_data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "packed_data = pack_padded_sequence(padded_data, lengths, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "111784d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi Ilango\\AppData\\Local\\Temp\\ipykernel_7064\\3918783295.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  targets = np.array([item[1] for item in batch],dtype=object)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([item[1] for item in batch],dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aabb3",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab521d1",
   "metadata": {},
   "source": [
    "In the implementation we define three objects: the encoder, the decoder and a full translation model that encapsulates the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "36fe2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo https://github.com/lkulowski/LSTM_encoder_decoder/blob/master/code/lstm_encoder_decoder.py\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, inp_size=2, hidden_size=110, n_layers=2, batch_size= batch_len, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.inp_size = inp_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = self.inp_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.n_layers,\n",
    "            dropout = dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of encoder model. It aims at\n",
    "        transforming the input sentence to a dense vector \n",
    "        \n",
    "        Input:\n",
    "        src shape:  [max_seq_len_in_batch, batch_size]\n",
    "\n",
    "        Output:\n",
    "        hidden and cell dense vectors (hidden and cell)\n",
    "        which contains all sentence information, shape [n layers, batch size, hid dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### Your code here ###\n",
    "        #src = [src len, batch size]\n",
    "\n",
    "        _, (hidden, cell) = self.rnn(X)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fdbf2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size =2, hid_dim=110, n_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        ### Your code here ###\n",
    "        # self.embedding = nn.Embedding(1, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, input_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        \n",
    "    def forward(self, input, encoder_hidden, encoder_cell):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder model. It aims at transforming\n",
    "        the dense representation of the encoder into a sentence in\n",
    "        the target language\n",
    "        \n",
    "        Input:\n",
    "        hidden shape: [n layers, batch size, hid dim]\n",
    "        cell shape: [n layers, batch size, hid dim]\n",
    "        input shape: [batch size]  # 1 token for each sentence in the batch\n",
    "        \n",
    "        Output:\n",
    "        prediction shape: [batch size, num_words_target_vocabulary]\n",
    "        hidden shape: [n layers, batch size, hid dim]\n",
    "        cell shape: [n layers, batch size, hid dim]\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        lstm_out, (self.hidden, self.cell) = self.rnn(input.unsqueeze(0), (encoder_hidden, encoder_cell))\n",
    "          \n",
    "        output = self.fc_out(lstm_out.squeeze(0))  # squeeze our 'sequence length 1' away\n",
    "        \n",
    "        return output, self.hidden, self.cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b3959378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class lstm_seq2seq(nn.Module):\n",
    "    '''Train LSTM encoder-decoder and make predictions '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "        # self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ration=0.5):\n",
    "        # calculate number of batch iterations\n",
    "        trg_len = trg.shape[0]\n",
    "        outputs = torch.zeros(trg_len, batch_len,2).to(device)\n",
    "\n",
    "        hidden,cell = self.encoder(src)\n",
    "\n",
    "        decoder_input = hidden[:,:,-1].T\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            decoder_output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio \n",
    "            decoder_input = trg[t,:,:] if teacher_force else decoder_output\n",
    "\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "30655e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(input, target, ignored_index, reduction='mean'):\n",
    "    mask = target == ignored_index\n",
    "    out = (input[~mask]-target[~mask])**2\n",
    "    if reduction == \"mean\":\n",
    "        return torch.sqrt(out.mean())\n",
    "    elif reduction == \"None\":\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce37af",
   "metadata": {},
   "source": [
    "### workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3c972f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_loader_32:\n",
    "    print(type(X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f80b1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = Encoder(batch_size=3, n_layers=2)\n",
    "en_model.train()\n",
    "en_model.to(device)\n",
    "for X,y in train_loader_32:\n",
    "    output = en_model(X)\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0c0748fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "batch_size = batch_len\n",
    "\n",
    "encoder = Encoder(n_layers=2).to(device)\n",
    "decoder = Decoder().to(device)\n",
    "teacher_forcing_ratio=0.5\n",
    "\n",
    "# criterion = nn.MSELoss(ignore_index = 0)\n",
    "\n",
    "for src, trg in train_loader_32:\n",
    "\n",
    "    trg_len = trg.shape[0]\n",
    "    outputs = torch.zeros(trg_len, batch_size,2).to(device)\n",
    "\n",
    "    hidden,cell = encoder(src)  \n",
    "    decoder_input = hidden[:,:,-1].T\n",
    "    # print(de_input.T.shape)\n",
    "\n",
    "    for t in range(1, trg_len):\n",
    "\n",
    "        decoder_output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        outputs[t] = decoder_output\n",
    "        teacher_force = random.random() < teacher_forcing_ratio \n",
    "        decoder_input = trg[t,:,:] if teacher_force else decoder_output\n",
    "        # break\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fae3f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "db3fce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(2, 110, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(2, 110, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=110, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo\n",
    "enc = encoder = Encoder(n_layers=2)\n",
    "dec = Decoder()\n",
    "\n",
    "seq2seq = lstm_seq2seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "seq2seq.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "seq2seq.train()\n",
    "CLIP =1\n",
    "clip = CLIP\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for src, trg in tqdm(train_loader_32):\n",
    "        seq2seq.optimizer.zero_grad()\n",
    "        output = seq2seq(src, trg)\n",
    "        loss = rmse_loss(output, trg, ignored_index=0)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), clip)\n",
    "        seq2seq.optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "        # break\n",
    "    print(epoch_loss/len(train_loader_32))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87278a2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cbb6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6f4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c10d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adc65bd17b90bc81ceb5a25ba99c9506844573e4b95946b15baa885a4f73e2ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
